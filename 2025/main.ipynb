{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdede78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GLOBAL CONFIG =====\n",
    "import pandas as pd\n",
    "\n",
    "YEAR = 2025\n",
    "\n",
    "# Input files\n",
    "GAME_INFO_FILE = f\"game_info_{YEAR}.csv\"\n",
    "BOX_PATH = f\"game_boxscores_{YEAR}.csv\"\n",
    "\n",
    "# Split outputs (based on dates)\n",
    "PRE_INFO_FILE  = f\"pre_info_{YEAR}.csv\"\n",
    "POST_INFO_FILE = f\"post_info_{YEAR}.csv\"\n",
    "\n",
    "# Team stats\n",
    "TEAM_STATS_FILE         = f\"team_stats_{YEAR}.csv\"\n",
    "TEAM_STATS_UPDATED = f\"team_stats_{YEAR}_updated.csv\"\n",
    "\n",
    "# Post-split with calculated diffs\n",
    "POST_DIFF_FILE = f\"post_info_{YEAR}_diff.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313d4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2025 split â†’ 1192 pre | 1489 post\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split game_info_<YEAR>.csv into:\n",
    "- pre_<YEAR>.csv  : games from Dec 1 (previous year) to Jan 9\n",
    "- post_<YEAR>.csv : games from Jan 10 to Mar 10 (inclusive)\n",
    "\"\"\"\n",
    "PRE_FILE  = f\"pre_{YEAR}.csv\"\n",
    "POST_FILE = f\"post_{YEAR}.csv\"\n",
    "\n",
    "# Define date boundaries\n",
    "PRE_START  = f\"{int(YEAR)-1}-02-07\"  # December 1 of previous year\n",
    "PRE_END    = f\"{YEAR}-02-06\"\n",
    "POST_START = f\"2022-11-07\"\n",
    "POST_END   = f\"{YEAR}-03-06\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(GAME_INFO_FILE)\n",
    "df[\"game_day\"] = pd.to_datetime(df[\"game_day\"], errors=\"coerce\")\n",
    "\n",
    "# Apply filters\n",
    "pre_df  = df[(df[\"game_day\"] >= PRE_START) & (df[\"game_day\"] <= PRE_END)]\n",
    "post_df = df[(df[\"game_day\"] >= POST_START) & (df[\"game_day\"] <= POST_END)]\n",
    "\n",
    "# Save files\n",
    "pre_df.to_csv(PRE_FILE, index=False)\n",
    "post_df.to_csv(POST_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… {YEAR} split â†’ {len(pre_df)} pre | {len(post_df)} post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ebeb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 339 teams â†’ team_stats_2025.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create team_stats_<YEAR>.csv with a single column 'team'\n",
    "containing teams that appear in both pre and post windows.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pre  = pd.read_csv(PRE_FILE)\n",
    "post = pd.read_csv(POST_FILE)\n",
    "\n",
    "teams_pre  = set(pre[\"home_team\"]).union(pre[\"away_team\"])\n",
    "teams_post = set(post[\"home_team\"]).union(post[\"away_team\"])\n",
    "\n",
    "common_teams = sorted(list(teams_pre & teams_post))\n",
    "pd.DataFrame({\"team\": common_teams}).to_csv(TEAM_STATS_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… {len(common_teams)} teams â†’ {TEAM_STATS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5607be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added home/away game_id lists â†’ team_stats_2025.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Augment team_stats_<YEAR>.csv with columns:\n",
    "- home_game_id: list of pre-season home game_ids\n",
    "- away_game_id: list of pre-season away game_ids\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "pre   = pd.read_csv(PRE_FILE)\n",
    "\n",
    "home_games = pre.groupby(\"home_team\")[\"game_id\"].apply(list).to_dict()\n",
    "away_games = pre.groupby(\"away_team\")[\"game_id\"].apply(list).to_dict()\n",
    "\n",
    "teams[\"home_game_id\"] = teams[\"team\"].map(home_games).apply(lambda x: x if isinstance(x, list) else [])\n",
    "teams[\"away_game_id\"] = teams[\"team\"].map(away_games).apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "teams.to_csv(TEAM_STATS_FILE, index=False)\n",
    "print(f\"âœ… Added home/away game_id lists â†’ {TEAM_STATS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26da0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Valid pre-game IDs in team_stats: 1192\n",
      "âœ… games_dict built for 1192 games | skipped: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build nested dictionary for valid games:\n",
    "games_dict[game_id][team_name] = {\n",
    "    \"team_total\": <one-row DataFrame of numeric totals>,\n",
    "    \"player_stats\": <player-level DataFrame for that team in that game>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, ast\n",
    "\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    s = str(x).strip()\n",
    "    if s in (\"\", \"[]\"): return []\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Load boxscores\n",
    "box = pd.read_csv(BOX_PATH)\n",
    "box[\"game_id\"] = pd.to_numeric(box[\"game_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "box = box.dropna(subset=[\"game_id\"]).copy()\n",
    "box[\"game_id\"] = box[\"game_id\"].astype(int)\n",
    "box[\"team\"]    = box[\"team\"].astype(str)\n",
    "\n",
    "# Load teams and collect valid game_ids\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_parse_list).apply(lambda L: [int(i) for i in L])\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_parse_list).apply(lambda L: [int(i) for i in L])\n",
    "\n",
    "valid_ids = set([gid for L in teams[\"home_game_id\"] for gid in L] +\n",
    "                [gid for L in teams[\"away_game_id\"] for gid in L])\n",
    "print(f\"ðŸŽ¯ Valid pre-game IDs in team_stats: {len(valid_ids)}\")\n",
    "\n",
    "# Filter box\n",
    "box = box[box[\"game_id\"].isin(valid_ids)].copy()\n",
    "\n",
    "# Build dict\n",
    "games_dict = {}\n",
    "skipped = 0\n",
    "\n",
    "for gid, gdf in box.groupby(\"game_id\"):\n",
    "    tm = gdf[\"team\"].dropna().unique()\n",
    "    if len(tm) != 2:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    games_dict[gid] = {}\n",
    "    numeric_cols = gdf.select_dtypes(include=\"number\").columns\n",
    "    for t in tm:\n",
    "        df_team = gdf[gdf[\"team\"] == t].copy()\n",
    "        team_total = df_team[numeric_cols].sum(numeric_only=True).to_frame().T.reset_index(drop=True)\n",
    "        games_dict[gid][t] = {\"team_total\": team_total, \"player_stats\": df_team.reset_index(drop=True)}\n",
    "\n",
    "print(f\"âœ… games_dict built for {len(games_dict)} games | skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ed4edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaned team_stats: removed 0 invalid game_id refs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove any game_ids in team_stats that are not present in games_dict.\n",
    "Overwrites team_stats_<YEAR>.csv with cleaned lists.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, ast\n",
    "\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_parse_list)\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_parse_list)\n",
    "\n",
    "games_dict_ids = set(map(int, games_dict.keys()))\n",
    "\n",
    "def _keep_known(L): return [int(g) for g in L if int(g) in games_dict_ids]\n",
    "\n",
    "before = teams[\"home_game_id\"].apply(len).sum() + teams[\"away_game_id\"].apply(len).sum()\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_keep_known)\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_keep_known)\n",
    "after  = teams[\"home_game_id\"].apply(len).sum() + teams[\"away_game_id\"].apply(len).sum()\n",
    "\n",
    "teams.to_csv(TEAM_STATS_FILE, index=False)\n",
    "print(f\"ðŸ§¹ Cleaned team_stats: removed {before - after} invalid game_id refs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2eb0d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Computed per-game eff/shooting for 1192 games, 2384 team-rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each (game, team) block in games_dict, compute:\n",
    "- poss  = FGA - OREB + TO + 0.475*FTA\n",
    "- off_eff = 100 * PTS / poss\n",
    "- def_eff = opponent's points per opponent poss\n",
    "Also store basic shooting metrics:\n",
    "- eFG% = (FGM + 0.5*3PM) / FGA\n",
    "- TS%  = PTS / [2 * (FGA + 0.44*FTA)]\n",
    "- 3PAr = 3PA / FGA\n",
    "- FTr  = FTA / FGA\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "FT_FACTOR = 0.475\n",
    "\n",
    "def _ensure(team_block, col):\n",
    "    tt = team_block[\"team_total\"]\n",
    "    if col not in tt.columns or pd.isna(tt[col].iloc[0]):\n",
    "        s = pd.to_numeric(team_block[\"player_stats\"].get(col, 0), errors=\"coerce\").fillna(0).sum()\n",
    "        if tt.empty:\n",
    "            team_block[\"team_total\"] = pd.DataFrame([{col: float(s)}])\n",
    "        else:\n",
    "            team_block[\"team_total\"].loc[:, col] = float(s)\n",
    "\n",
    "def _compute_shooting(tt):\n",
    "    # guard zeros\n",
    "    FGA = float(tt.get(\"fga\", [0])[0] if \"fga\" in tt else 0)\n",
    "    FGM = float(tt.get(\"fgm\", [0])[0] if \"fgm\" in tt else 0)\n",
    "    P3M = float(tt.get(\"3pm\", [0])[0] if \"3pm\" in tt else 0)\n",
    "    P3A = float(tt.get(\"3pa\", [0])[0] if \"3pa\" in tt else 0)\n",
    "    FTA = float(tt.get(\"fta\", [0])[0] if \"fta\" in tt else 0)\n",
    "    efg = (FGM + 0.5*P3M) / FGA if FGA > 0 else np.nan\n",
    "    ts  = (tt[\"pts\"].iloc[0]) / (2 * (FGA + 0.44*FTA)) if (FGA + 0.44*FTA) > 0 else np.nan\n",
    "    threepar = P3A / FGA if FGA > 0 else np.nan\n",
    "    ftr  = FTA / FGA if FGA > 0 else np.nan\n",
    "    return efg, ts, threepar, ftr\n",
    "\n",
    "rows = []\n",
    "for gid, teams_block in games_dict.items():\n",
    "    if len(teams_block) != 2: continue\n",
    "    t1, t2 = list(teams_block.keys())\n",
    "\n",
    "    for t in (t1, t2):\n",
    "        for c in [\"fga\",\"oreb\",\"to\",\"fta\",\"pts\",\"fgm\",\"3pm\",\"3pa\"]:\n",
    "            _ensure(teams_block[t], c)\n",
    "\n",
    "    tt1, tt2 = teams_block[t1][\"team_total\"], teams_block[t2][\"team_total\"]\n",
    "\n",
    "    for t_cur, t_opp in [(t1, t2), (t2, t1)]:\n",
    "        cur = teams_block[t_cur][\"team_total\"].copy()\n",
    "        opp = teams_block[t_opp][\"team_total\"].copy()\n",
    "\n",
    "        fga, oreb, tov, fta = [float(pd.to_numeric(cur[c], errors=\"coerce\").iloc[0]) for c in [\"fga\",\"oreb\",\"to\",\"fta\"]]\n",
    "        poss = fga - oreb + tov + FT_FACTOR * fta\n",
    "        pts  = float(pd.to_numeric(cur[\"pts\"], errors=\"coerce\").iloc[0])\n",
    "\n",
    "        poss_opp = float(pd.to_numeric(opp[\"fga\"], errors=\"coerce\").iloc[0] -\n",
    "                         pd.to_numeric(opp[\"oreb\"], errors=\"coerce\").iloc[0] +\n",
    "                         pd.to_numeric(opp[\"to\"], errors=\"coerce\").iloc[0] +\n",
    "                         FT_FACTOR * pd.to_numeric(opp[\"fta\"], errors=\"coerce\").iloc[0])\n",
    "\n",
    "        off_eff = 100.0 * pts / poss if poss > 0 else np.nan\n",
    "        def_eff = 100.0 * float(pd.to_numeric(opp[\"pts\"], errors=\"coerce\").iloc[0]) / poss_opp if poss_opp > 0 else np.nan\n",
    "\n",
    "        efg, ts, threepar, ftr = _compute_shooting(cur)\n",
    "\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"poss\"]    = poss\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"off_eff\"] = off_eff\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"def_eff\"] = def_eff\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"efg_pct\"] = efg\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"ts_pct\"]  = ts\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"threepar\"]= threepar\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"ftr\"]     = ftr\n",
    "\n",
    "        rows.append({\"game_id\": gid, \"team\": t_cur, \"opp\": t_opp,\n",
    "                     \"off_eff\": off_eff, \"def_eff\": def_eff,\n",
    "                     \"efg_pct\": efg, \"ts_pct\": ts, \"threepar\": threepar, \"ftr\": ftr})\n",
    "\n",
    "eff_df = pd.DataFrame(rows)\n",
    "print(f\"âœ… Computed per-game eff/shooting for {eff_df['game_id'].nunique()} games, {eff_df.shape[0]} team-rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db84498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved team season averages (raw & adjusted) â†’ team_stats_2025_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create per-team season averages and adjusted efficiencies,\n",
    "then save to team_stats_<YEAR>_updated.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Team means\n",
    "team_means = eff_df.groupby(\"team\")[[\"off_eff\",\"def_eff\",\"efg_pct\",\"ts_pct\",\"threepar\",\"ftr\"]].mean()\n",
    "\n",
    "# League means for scaling\n",
    "league_off = team_means[\"off_eff\"].mean()\n",
    "league_def = team_means[\"def_eff\"].mean()\n",
    "\n",
    "# Build opponent averages for adjustment\n",
    "opp_means = eff_df.groupby(\"team\")[[\"off_eff\",\"def_eff\"]].mean().rename(\n",
    "    columns={\"off_eff\":\"avg_off_eff\",\"def_eff\":\"avg_def_eff\"}\n",
    ")\n",
    "\n",
    "eff_adj = eff_df.merge(\n",
    "    opp_means.rename(columns={\"avg_off_eff\":\"avg_off_eff_opp\", \"avg_def_eff\":\"avg_def_eff_opp\"}),\n",
    "    left_on=\"opp\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "eff_adj[\"adj_off_eff\"] = (eff_adj[\"off_eff\"] / eff_adj[\"avg_def_eff_opp\"]) * league_off\n",
    "eff_adj[\"adj_def_eff\"] = (eff_adj[\"def_eff\"] / eff_adj[\"avg_off_eff_opp\"]) * league_def\n",
    "\n",
    "# Season averages (adjusted + raw shooting)\n",
    "team_adj_means = eff_adj.groupby(\"team\")[[\"adj_off_eff\",\"adj_def_eff\"]].mean()\n",
    "team_stats_means = team_means.join(team_adj_means, how=\"left\")\n",
    "\n",
    "# Merge into team_stats file\n",
    "base = pd.read_csv(TEAM_STATS_FILE)\n",
    "team_stats_updated = base.merge(\n",
    "    team_stats_means.reset_index(),\n",
    "    on=\"team\", how=\"left\"\n",
    ")\n",
    "\n",
    "team_stats_updated.to_csv(TEAM_STATS_UPDATED, index=False)\n",
    "print(f\"âœ… Saved team season averages (raw & adjusted) â†’ {TEAM_STATS_UPDATED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08a447b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched post_2025.csv with season stats, net ratings, and luck\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute team luck from post_<YEAR>.csv and merge (luck_hometeam / luck_awayteam)\n",
    "Then also compute net_rating for each side.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "post = pd.read_csv(POST_FILE)\n",
    "\n",
    "# point spread & booleans (ensure numeric)\n",
    "post[\"point_spread\"] = post[\"home_score\"] - post[\"away_score\"]\n",
    "\n",
    "# Build per-team win/margin from post window\n",
    "post[\"home_win\"] = (post[\"point_spread\"] > 0).astype(int)\n",
    "\n",
    "home_df = post.copy()\n",
    "home_df[\"team\"] = home_df[\"home_team\"]\n",
    "home_df[\"margin\"] = home_df[\"point_spread\"]\n",
    "home_df[\"win\"] = home_df[\"home_win\"]\n",
    "\n",
    "away_df = post.copy()\n",
    "away_df[\"team\"] = away_df[\"away_team\"]\n",
    "away_df[\"margin\"] = -away_df[\"point_spread\"]\n",
    "away_df[\"win\"] = 1 - away_df[\"home_win\"]\n",
    "\n",
    "team_games = pd.concat([home_df, away_df], ignore_index=True)\n",
    "\n",
    "summary = (team_games.groupby(\"team\")\n",
    "           .agg(actual_win_pct=(\"win\",\"mean\"),\n",
    "                avg_margin=(\"margin\",\"mean\"))\n",
    "           .reset_index())\n",
    "\n",
    "summary[\"expected_win_pct\"] = 1 / (1 + 10 ** (-summary[\"avg_margin\"] / 10))\n",
    "summary[\"luck\"] = summary[\"actual_win_pct\"] - summary[\"expected_win_pct\"]\n",
    "\n",
    "luck = summary[[\"team\",\"luck\"]]\n",
    "\n",
    "# Merge season efficiencies from updated team file\n",
    "stats = pd.read_csv(TEAM_STATS_UPDATED)\n",
    "\n",
    "# Build home/away versions of season stats you want on POST_FILE\n",
    "home_cols = {\n",
    "    \"team\":\"home_team\",\n",
    "    \"off_eff\":\"off_eff_hometeam\",\n",
    "    \"def_eff\":\"def_eff_hometeam\",\n",
    "    \"adj_off_eff\":\"adj_off_eff_hometeam\",\n",
    "    \"adj_def_eff\":\"adj_def_eff_hometeam\",\n",
    "    \"efg_pct\":\"efg_pct_hometeam\",\n",
    "    \"ts_pct\":\"ts_pct_hometeam\",\n",
    "    \"threepar\":\"threepar_hometeam\",\n",
    "    \"ftr\":\"ftr_hometeam\"\n",
    "}\n",
    "away_cols = {k: v.replace(\"home\",\"away\") for k, v in home_cols.items()}\n",
    "\n",
    "post_enriched = post.merge(stats.rename(columns=home_cols), on=\"home_team\", how=\"left\")\n",
    "post_enriched = post_enriched.merge(stats.rename(columns=away_cols), on=\"away_team\", how=\"left\")\n",
    "\n",
    "# Net ratings\n",
    "post_enriched[\"net_rating_hometeam\"] = post_enriched[\"adj_off_eff_hometeam\"] - post_enriched[\"adj_def_eff_hometeam\"]\n",
    "post_enriched[\"net_rating_awayteam\"] = post_enriched[\"adj_off_eff_awayteam\"] - post_enriched[\"adj_def_eff_awayteam\"]\n",
    "\n",
    "# Luck (home/away)\n",
    "post_enriched = post_enriched.merge(luck.rename(columns={\"team\":\"home_team\",\"luck\":\"luck_hometeam\"}), on=\"home_team\", how=\"left\")\n",
    "post_enriched = post_enriched.merge(luck.rename(columns={\"team\":\"away_team\",\"luck\":\"luck_awayteam\"}), on=\"away_team\", how=\"left\")\n",
    "\n",
    "post_enriched.to_csv(POST_FILE, index=False)\n",
    "print(f\"âœ… Enriched {POST_FILE} with season stats, net ratings, and luck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3447e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added 'home_advantage' to POST_FILE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute home_advantage_score per team from pre window and add to POST and POST_DIFF later.\n",
    "home_advantage_score = avg_home_margin - avg_away_margin (excluding neutral)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pre = pd.read_csv(PRE_FILE)\n",
    "if \"is_neutral\" in pre.columns:\n",
    "    pre = pre[pre[\"is_neutral\"] == False]\n",
    "\n",
    "pre[\"home_margin\"] = pre[\"home_score\"] - pre[\"away_score\"]\n",
    "\n",
    "home_avg = pre.groupby(\"home_team\")[\"home_margin\"].mean().reset_index().rename(\n",
    "    columns={\"home_team\":\"team\",\"home_margin\":\"avg_home_margin\"}\n",
    ")\n",
    "away_avg = pre.groupby(\"away_team\")[\"home_margin\"].mean().reset_index().rename(\n",
    "    columns={\"away_team\":\"team\",\"home_margin\":\"avg_away_margin\"}\n",
    ")\n",
    "\n",
    "adv = pd.merge(home_avg, away_avg, on=\"team\", how=\"outer\").fillna(0)\n",
    "adv[\"home_advantage_score\"] = adv[\"avg_home_margin\"] - adv[\"avg_away_margin\"]\n",
    "\n",
    "# Merge only the home team's advantage into POST_FILE\n",
    "post = pd.read_csv(POST_FILE)\n",
    "post = post.merge(adv[[\"team\",\"home_advantage_score\"]].rename(columns={\"team\":\"home_team\",\"home_advantage_score\":\"home_advantage\"}),\n",
    "                  on=\"home_team\", how=\"left\")\n",
    "post.to_csv(POST_FILE, index=False)\n",
    "print(\"âœ… Added 'home_advantage' to POST_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf77494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved diff dataset â†’ post_info_2025_diff.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute home - away differences for key features and save post_<YEAR>_diff.csv.\n",
    "Keeps core game info + diff columns + (optional) home_advantage.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(POST_FILE)\n",
    "\n",
    "# Ensure booleans are numeric\n",
    "for c in [\"is_conference\",\"is_neutral\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "# Differences\n",
    "pairs = [\n",
    "    (\"off_eff_hometeam\", \"off_eff_awayteam\"),\n",
    "    (\"def_eff_hometeam\", \"def_eff_awayteam\"),\n",
    "    (\"adj_off_eff_hometeam\", \"adj_off_eff_awayteam\"),\n",
    "    (\"adj_def_eff_hometeam\", \"adj_def_eff_awayteam\"),\n",
    "    (\"efg_pct_hometeam\", \"efg_pct_awayteam\"),\n",
    "    (\"ts_pct_hometeam\", \"ts_pct_awayteam\"),\n",
    "    (\"threepar_hometeam\", \"threepar_awayteam\"),\n",
    "    (\"ftr_hometeam\", \"ftr_awayteam\"),\n",
    "    (\"luck_hometeam\", \"luck_awayteam\"),\n",
    "    (\"net_rating_hometeam\",\"net_rating_awayteam\"),\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    if a in df.columns and b in df.columns:\n",
    "        df[a.replace(\"_hometeam\",\"_diff\")] = df[a] - df[b]\n",
    "\n",
    "# Select columns to keep\n",
    "keep = [\n",
    "    \"home_team\",\"away_team\",\"home_score\",\"away_score\",\"point_spread\",\n",
    "    \"is_conference\",\"is_neutral\",\"home_advantage\"  # optional if computed\n",
    "] + [c for c in df.columns if c.endswith(\"_diff\")]\n",
    "\n",
    "df = df[keep]\n",
    "df.to_csv(POST_DIFF_FILE, index=False)\n",
    "print(f\"âœ… Saved diff dataset â†’ {POST_DIFF_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85826c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added interaction terms â†’ post_info_2025_diff.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optional nonlinearity: add interaction terms to post_<YEAR>_diff.csv\n",
    "- rating_luck_interaction = net_rating_diff * luck_diff\n",
    "- shooting_strength_interaction = net_rating_diff * efg_pct_diff\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(POST_DIFF_FILE)\n",
    "for req in [\"net_rating_diff\",\"luck_diff\",\"efg_pct_diff\"]:\n",
    "    if req not in df.columns:\n",
    "        raise ValueError(f\"Missing {req} in {POST_DIFF_FILE}\")\n",
    "\n",
    "df[\"rating_luck_interaction\"] = df[\"net_rating_diff\"] * df[\"luck_diff\"]\n",
    "df[\"shooting_strength_interaction\"] = df[\"net_rating_diff\"] * df[\"efg_pct_diff\"]\n",
    "df.to_csv(POST_DIFF_FILE, index=False)\n",
    "print(f\"âœ… Added interaction terms â†’ {POST_DIFF_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b54005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 9 features: ['net_rating_diff', 'efg_pct_diff', 'luck_diff', 'adj_off_eff_diff', 'home_advantage', 'rating_luck_interaction', 'shooting_strength_interaction', 'is_conference', 'is_neutral']\n",
      "âœ… Train size: 1031 | Test size: 442\n"
     ]
    }
   ],
   "source": [
    "# === 1) SETUP & SPLIT ================================================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load diff dataset from your global POST_DIFF_FILE\n",
    "df = pd.read_csv(POST_DIFF_FILE)\n",
    "\n",
    "# Choose features & target\n",
    "features = [\n",
    "    \"net_rating_diff\",\n",
    "    \"efg_pct_diff\",\n",
    "    \"luck_diff\",\n",
    "    \"adj_off_eff_diff\",\n",
    "    \"home_advantage\",               \n",
    "    \"rating_luck_interaction\",      \n",
    "    \"shooting_strength_interaction\",\n",
    "    \"is_conference\",\n",
    "    \"is_neutral\",\n",
    "]\n",
    "# Keep only columns that actually exist\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "TARGET = \"point_spread\"\n",
    "df = df.dropna(subset=features + [TARGET])\n",
    "\n",
    "# Split features/target\n",
    "X = df[features]\n",
    "y = df[TARGET]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Scale (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ… Using {len(features)} features: {features}\")\n",
    "print(f\"âœ… Train size: {X_train.shape[0]} | Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d504e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ€ Model Performance Comparison:\n",
      "           Model      MAE       RÂ²\n",
      "  HuberRegressor 8.273064 0.605235\n",
      "           Ridge 8.295803 0.604820\n",
      "      ElasticNet 8.297378 0.604729\n",
      "LinearRegression 8.298796 0.604646\n",
      "           Lasso 8.299090 0.604629\n",
      "    RandomForest 8.970935 0.559730\n",
      "GradientBoosting 8.978402 0.560075\n",
      "\n",
      "ðŸŒŸ Best by MAE: HuberRegressor\n"
     ]
    }
   ],
   "source": [
    "# === 2) MODEL TRAINING ===============================================\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.001, max_iter=10000),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000),\n",
    "    \"HuberRegressor\": HuberRegressor(epsilon=1.35, max_iter=1000),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=42),\n",
    "}\n",
    "\n",
    "results, trained = [], {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained[name] = model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"RÂ²\": r2_score(y_test, y_pred),\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\nðŸ€ Model Performance Comparison:\")\n",
    "print(res.to_string(index=False))\n",
    "\n",
    "# Keep the best model name for analysis chunk\n",
    "best_model_name = res.iloc[0][\"Model\"]\n",
    "print(f\"\\nðŸŒŸ Best by MAE: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ccfe375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Feature correlation matrix (train set):\n",
      "                               net_rating_diff  efg_pct_diff  luck_diff  \\\n",
      "net_rating_diff                          1.000         0.759     -0.532   \n",
      "efg_pct_diff                             0.759         1.000     -0.484   \n",
      "luck_diff                               -0.532        -0.484      1.000   \n",
      "adj_off_eff_diff                         0.877         0.880     -0.490   \n",
      "home_advantage                           0.879         0.720     -0.474   \n",
      "rating_luck_interaction                  0.553         0.477     -0.459   \n",
      "shooting_strength_interaction           -0.724        -0.786      0.308   \n",
      "is_conference                              NaN           NaN        NaN   \n",
      "is_neutral                               0.157         0.141     -0.101   \n",
      "\n",
      "                               adj_off_eff_diff  home_advantage  \\\n",
      "net_rating_diff                           0.877           0.879   \n",
      "efg_pct_diff                              0.880           0.720   \n",
      "luck_diff                                -0.490          -0.474   \n",
      "adj_off_eff_diff                          1.000           0.798   \n",
      "home_advantage                            0.798           1.000   \n",
      "rating_luck_interaction                   0.501           0.515   \n",
      "shooting_strength_interaction            -0.763          -0.653   \n",
      "is_conference                               NaN             NaN   \n",
      "is_neutral                                0.157           0.150   \n",
      "\n",
      "                               rating_luck_interaction  \\\n",
      "net_rating_diff                                  0.553   \n",
      "efg_pct_diff                                     0.477   \n",
      "luck_diff                                       -0.459   \n",
      "adj_off_eff_diff                                 0.501   \n",
      "home_advantage                                   0.515   \n",
      "rating_luck_interaction                          1.000   \n",
      "shooting_strength_interaction                   -0.630   \n",
      "is_conference                                      NaN   \n",
      "is_neutral                                       0.064   \n",
      "\n",
      "                               shooting_strength_interaction  is_conference  \\\n",
      "net_rating_diff                                       -0.724            NaN   \n",
      "efg_pct_diff                                          -0.786            NaN   \n",
      "luck_diff                                              0.308            NaN   \n",
      "adj_off_eff_diff                                      -0.763            NaN   \n",
      "home_advantage                                        -0.653            NaN   \n",
      "rating_luck_interaction                               -0.630            NaN   \n",
      "shooting_strength_interaction                          1.000            NaN   \n",
      "is_conference                                            NaN            NaN   \n",
      "is_neutral                                            -0.120            NaN   \n",
      "\n",
      "                               is_neutral  \n",
      "net_rating_diff                     0.157  \n",
      "efg_pct_diff                        0.141  \n",
      "luck_diff                          -0.101  \n",
      "adj_off_eff_diff                    0.157  \n",
      "home_advantage                      0.150  \n",
      "rating_luck_interaction             0.064  \n",
      "shooting_strength_interaction      -0.120  \n",
      "is_conference                         NaN  \n",
      "is_neutral                          1.000  \n",
      "\n",
      "ðŸ”Ž Top coefficients (LinearRegression) on standardized features:\n",
      "net_rating_diff                  12.525641\n",
      "shooting_strength_interaction     0.870068\n",
      "luck_diff                        -0.857071\n",
      "rating_luck_interaction           0.836259\n",
      "efg_pct_diff                      0.534774\n",
      "adj_off_eff_diff                  0.370347\n",
      "is_neutral                        0.353570\n",
      "home_advantage                    0.262365\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (Ridge) on standardized features:\n",
      "net_rating_diff                  12.431832\n",
      "luck_diff                        -0.867673\n",
      "shooting_strength_interaction     0.851244\n",
      "rating_luck_interaction           0.834859\n",
      "efg_pct_diff                      0.513565\n",
      "adj_off_eff_diff                  0.417395\n",
      "is_neutral                        0.353806\n",
      "home_advantage                    0.305662\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (Lasso) on standardized features:\n",
      "net_rating_diff                  12.523794\n",
      "shooting_strength_interaction     0.862211\n",
      "luck_diff                        -0.858560\n",
      "rating_luck_interaction           0.832981\n",
      "efg_pct_diff                      0.530311\n",
      "adj_off_eff_diff                  0.369612\n",
      "is_neutral                        0.352658\n",
      "home_advantage                    0.262768\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (ElasticNet) on standardized features:\n",
      "net_rating_diff                  12.476142\n",
      "luck_diff                        -0.863311\n",
      "shooting_strength_interaction     0.856400\n",
      "rating_luck_interaction           0.833890\n",
      "efg_pct_diff                      0.521520\n",
      "adj_off_eff_diff                  0.394394\n",
      "is_neutral                        0.353235\n",
      "home_advantage                    0.284989\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (HuberRegressor) on standardized features:\n",
      "net_rating_diff                  12.334791\n",
      "rating_luck_interaction           1.082592\n",
      "shooting_strength_interaction     0.872729\n",
      "luck_diff                        -0.682841\n",
      "efg_pct_diff                      0.502528\n",
      "home_advantage                    0.444173\n",
      "is_neutral                        0.364939\n",
      "adj_off_eff_diff                  0.187900\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸŒ² Feature importances (RandomForest):\n",
      "net_rating_diff                  0.695254\n",
      "home_advantage                   0.065434\n",
      "adj_off_eff_diff                 0.055723\n",
      "rating_luck_interaction          0.052138\n",
      "luck_diff                        0.047723\n",
      "shooting_strength_interaction    0.041485\n",
      "efg_pct_diff                     0.039533\n",
      "is_neutral                       0.002710\n",
      "is_conference                    0.000000\n",
      "\n",
      "ðŸŒ² Feature importances (GradientBoosting):\n",
      "net_rating_diff                  0.762556\n",
      "home_advantage                   0.060400\n",
      "rating_luck_interaction          0.047674\n",
      "luck_diff                        0.041670\n",
      "adj_off_eff_diff                 0.036125\n",
      "shooting_strength_interaction    0.025859\n",
      "efg_pct_diff                     0.024876\n",
      "is_neutral                       0.000839\n",
      "is_conference                    0.000000\n",
      "\n",
      "ðŸŽ¯ Permutation importance (Î”MAE when shuffled) for HuberRegressor:\n",
      "luck_diff                        0.026308\n",
      "adj_off_eff_diff                 0.002041\n",
      "is_conference                   -0.000000\n",
      "efg_pct_diff                    -0.002579\n",
      "home_advantage                  -0.010778\n",
      "rating_luck_interaction         -0.060282\n",
      "is_neutral                      -0.061396\n",
      "shooting_strength_interaction   -0.115020\n",
      "net_rating_diff                 -7.522305\n"
     ]
    }
   ],
   "source": [
    "# === 3) ANALYSIS ======================================================\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nðŸ“Œ Feature correlation matrix (train set):\")\n",
    "corr = pd.DataFrame(X_train, columns=features).corr()\n",
    "print(corr.round(3))\n",
    "\n",
    "# Coefficients for linear-type models (on standardized features)\n",
    "def show_linear_coefs(name):\n",
    "    if name in trained and hasattr(trained[name], \"coef_\"):\n",
    "        coefs = pd.Series(trained[name].coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "        print(f\"\\nðŸ”Ž Top coefficients ({name}) on standardized features:\")\n",
    "        print(coefs.to_string())\n",
    "    else:\n",
    "        print(f\"\\n({name}) has no linear coefficients to display.\")\n",
    "\n",
    "for lin in [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"HuberRegressor\"]:\n",
    "    show_linear_coefs(lin)\n",
    "\n",
    "# Tree-based importance (if available)\n",
    "def show_importance(name):\n",
    "    if name in trained and hasattr(trained[name], \"feature_importances_\"):\n",
    "        imps = pd.Series(trained[name].feature_importances_, index=features).sort_values(ascending=False)\n",
    "        print(f\"\\nðŸŒ² Feature importances ({name}):\")\n",
    "        print(imps.to_string())\n",
    "    else:\n",
    "        print(f\"\\n({name}) has no tree feature_importances_ to display.\")\n",
    "\n",
    "for tree in [\"RandomForest\", \"GradientBoosting\"]:\n",
    "    show_importance(tree)\n",
    "\n",
    "# Optional: permutation importance for the best model (more robust)\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    best_model = trained[best_model_name]\n",
    "    perm = permutation_importance(best_model, X_test_scaled, y_test, scoring=\"neg_mean_absolute_error\",\n",
    "                                  n_repeats=10, random_state=42)\n",
    "    perm_imps = pd.Series(-perm.importances_mean, index=features).sort_values(ascending=False)\n",
    "    print(f\"\\nðŸŽ¯ Permutation importance (Î”MAE when shuffled) for {best_model_name}:\")\n",
    "    print(perm_imps.to_string())\n",
    "except Exception as e:\n",
    "    print(f\"\\n(perm importance skipped) {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114fc87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Learning Curve (MAE Â± sd)\n",
      "Size   Best_MAE  Â±sd     Ridge_MAE  Â±sd\n",
      " 10%      8.658 Â±0.207        8.665 Â±0.212\n",
      " 20%      8.417 Â±0.123        8.417 Â±0.125\n",
      " 30%      8.257 Â±0.070        8.286 Â±0.066\n",
      " 40%      8.369 Â±0.123        8.370 Â±0.104\n",
      " 50%      8.318 Â±0.062        8.334 Â±0.094\n",
      " 60%      8.294 Â±0.036        8.320 Â±0.039\n",
      " 70%      8.315 Â±0.018        8.338 Â±0.024\n",
      " 80%      8.289 Â±0.030        8.303 Â±0.025\n",
      " 90%      8.279 Â±0.028        8.299 Â±0.029\n",
      "100%      8.273 Â±0.000        8.296 Â±0.000\n",
      "\n",
      "ðŸ§­ Diagnosis:\n",
      "- Best model improvement 50%â†’100%: 0.045 MAE\n",
      "- Ridge improvement 50%â†’100%     : 0.038 MAE\n",
      "- Last-step improvement (best)    : 0.006 MAE\n",
      "\n",
      "âœ… Verdict: ðŸ§ª Curve is flat: focus on richer features (tempo/recent form/rest/travel/etc.).\n"
     ]
    }
   ],
   "source": [
    "# === 2.5) LEARNING CURVE (plug-and-play) ==============================\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import clone\n",
    "\n",
    "def learning_curve_mae(model, Xtr, ytr, Xte, yte, sizes=None, repeats=8, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains `model` on increasing fractions of (Xtr, ytr) and evaluates MAE on (Xte, yte).\n",
    "    Returns sizes, mean MAE per size, and std MAE per size.\n",
    "    \"\"\"\n",
    "    if sizes is None:\n",
    "        sizes = np.linspace(0.1, 1.0, 10)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    means, stds = [], []\n",
    "    ytr_np = ytr.to_numpy() if hasattr(ytr, \"to_numpy\") else np.asarray(ytr)\n",
    "    for s in sizes:\n",
    "        n = max(10, int(len(Xtr) * s))\n",
    "        maes = []\n",
    "        for _ in range(repeats):\n",
    "            idx = rng.choice(len(Xtr), n, replace=False)\n",
    "            m = clone(model)\n",
    "            m.fit(Xtr[idx], ytr_np[idx])\n",
    "            pred = m.predict(Xte)\n",
    "            maes.append(mean_absolute_error(yte, pred))\n",
    "        means.append(float(np.mean(maes)))\n",
    "        stds.append(float(np.std(maes)))\n",
    "    return np.array(sizes), np.array(means), np.array(stds)\n",
    "\n",
    "# Pick the best model from your leaderboard (already computed above)\n",
    "best_model = trained[best_model_name]\n",
    "\n",
    "# Run learning curves for the best model and a Ridge baseline\n",
    "sizes, best_mean, best_std = learning_curve_mae(best_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "_,     ridge_mean, ridge_std = learning_curve_mae(Ridge(alpha=1.0), X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Pretty print\n",
    "print(\"\\nðŸ“ˆ Learning Curve (MAE Â± sd)\")\n",
    "print(\"Size   Best_MAE  Â±sd     Ridge_MAE  Â±sd\")\n",
    "for s, bm, bs, rm, rs in zip(sizes, best_mean, best_std, ridge_mean, ridge_std):\n",
    "    print(f\"{int(s*100):>3d}%   {bm:8.3f} Â±{bs:4.3f}   {rm:10.3f} Â±{rs:4.3f}\")\n",
    "\n",
    "# Heuristic verdict\n",
    "def improvement(means, sizes_arr):\n",
    "    # improvement from ~50% to 100%\n",
    "    mid_idx = np.argmin(np.abs(sizes_arr - 0.5))\n",
    "    return means[mid_idx] - means[-1]\n",
    "\n",
    "best_improve = improvement(best_mean, sizes)\n",
    "ridge_improve = improvement(ridge_mean, sizes)\n",
    "tail_slope = best_mean[-2] - best_mean[-1] if len(best_mean) > 1 else 0.0\n",
    "\n",
    "print(\"\\nðŸ§­ Diagnosis:\")\n",
    "print(f\"- Best model improvement 50%â†’100%: {best_improve:.3f} MAE\")\n",
    "print(f\"- Ridge improvement 50%â†’100%     : {ridge_improve:.3f} MAE\")\n",
    "print(f\"- Last-step improvement (best)    : {tail_slope:.3f} MAE\")\n",
    "\n",
    "if best_improve >= 0.30 or tail_slope > 0.05:\n",
    "    verdict = \"ðŸ“¦ More data likely helps (curve still descending).\"\n",
    "elif best_improve < 0.10 and abs(tail_slope) < 0.02:\n",
    "    verdict = \"ðŸ§ª Curve is flat: focus on richer features (tempo/recent form/rest/travel/etc.).\"\n",
    "else:\n",
    "    verdict = \"âš–ï¸ Mixed: modest data gains possible, but new features will likely matter more.\"\n",
    "\n",
    "print(f\"\\nâœ… Verdict: {verdict}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
