{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42012c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 4 files into 5,293 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Folder where your CSVs live (use \".\" for current folder)\n",
    "folder = Path(\".\")\n",
    "\n",
    "files = sorted(folder.glob(\"post_info_*_diff.csv\"))\n",
    "\n",
    "# Read and combine\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# (Optional) add a year column extracted from filename\n",
    "combined[\"year\"] = [f.stem.split(\"_\")[2] for f in files for _ in range(len(pd.read_csv(f)))]\n",
    "\n",
    "# Save\n",
    "combined.to_csv(\"post_info_2023_2026_diff_combined.csv\", index=False)\n",
    "\n",
    "print(f\"Combined {len(files)} files into {len(combined):,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae8e8c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ HOLDOUT (Train 2023‚Äì2025, Test 2026) ================\n",
      "\n",
      "Best params (GB tuned, GroupKFold): {'model__subsample': 0.8, 'model__n_estimators': 300, 'model__min_samples_leaf': 15, 'model__max_depth': 3, 'model__learning_rate': 0.03}\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_LinearRegression.joblib | MAE=8.3593 R2=0.7165\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_Ridge.joblib | MAE=8.3597 R2=0.7165\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_Lasso.joblib | MAE=8.3591 R2=0.7165\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_ElasticNet.joblib | MAE=8.3601 R2=0.7164\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_HuberRegressor.joblib | MAE=8.3740 R2=0.7147\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_RandomForest.joblib | MAE=8.5661 R2=0.6993\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_GradientBoosting_TUNED.joblib | MAE=8.5742 R2=0.6952\n",
      "\n",
      "üèÄ Holdout Performance Ranking:\n",
      "                 Model      MAE       R2\n",
      "                 Lasso 8.359093 0.716519\n",
      "      LinearRegression 8.359286 0.716507\n",
      "                 Ridge 8.359728 0.716475\n",
      "            ElasticNet 8.360064 0.716449\n",
      "        HuberRegressor 8.374019 0.714744\n",
      "          RandomForest 8.566120 0.699305\n",
      "GradientBoosting_TUNED 8.574240 0.695209\n",
      "\n",
      "üåü Best by MAE (holdout): Lasso\n",
      "\n",
      "================ RANDOM SPLIT (Mixed 2023‚Äì2026) ================\n",
      "\n",
      "‚úÖ Total rows: 7,962\n",
      "‚úÖ Train rows: 6,369 | Test rows: 1,593\n",
      "‚úÖ Features (9): ['net_rating_diff', 'efg_pct_diff', 'luck_diff', 'adj_off_eff_diff', 'home_advantage', 'rating_luck_interaction', 'shooting_strength_interaction', 'is_conference', 'is_neutral']\n",
      "‚úÖ Numeric scaled: ['net_rating_diff', 'efg_pct_diff', 'luck_diff', 'adj_off_eff_diff', 'rating_luck_interaction', 'shooting_strength_interaction']\n",
      "‚úÖ Left unscaled (flags): ['is_conference', 'is_neutral', 'home_advantage']\n",
      "Best params (GB tuned, KFold): {'model__subsample': 0.8, 'model__n_estimators': 600, 'model__min_samples_leaf': 15, 'model__max_depth': 3, 'model__learning_rate': 0.03}\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_LinearRegression.joblib | MAE=8.1016 R2=0.6607\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_Ridge.joblib | MAE=8.1014 R2=0.6607\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_Lasso.joblib | MAE=8.1015 R2=0.6607\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_ElasticNet.joblib | MAE=8.1011 R2=0.6607\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_HuberRegressor.joblib | MAE=8.0803 R2=0.6607\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_RandomForest.joblib | MAE=8.0483 R2=0.6585\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_GradientBoosting_TUNED.joblib | MAE=7.8888 R2=0.6671\n",
      "\n",
      "üèÄ Random-Split Performance Ranking:\n",
      "                 Model      MAE       R2\n",
      "GradientBoosting_TUNED 7.888815 0.667123\n",
      "          RandomForest 8.048282 0.658453\n",
      "        HuberRegressor 8.080316 0.660740\n",
      "            ElasticNet 8.101133 0.660709\n",
      "                 Ridge 8.101432 0.660670\n",
      "                 Lasso 8.101493 0.660667\n",
      "      LinearRegression 8.101564 0.660655\n",
      "\n",
      "üåü Best by MAE (random): GradientBoosting_TUNED\n",
      "\n",
      "‚úÖ Done. All models saved into:\n",
      " - models_holdout/ (holdout bundles)\n",
      " - models_random/  (random-split bundles)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TRAIN + SAVE ALL MODELS (HOLDOUT + RANDOM SPLIT)\n",
    "\n",
    "What this script does:\n",
    "\n",
    "A) HOLDOUT (season-based):\n",
    "   - Train on 2023‚Äì2025, test on 2026\n",
    "   - Tune GradientBoosting with GroupKFold (groups=season)\n",
    "   - Evaluate all models on 2026\n",
    "   - Save *every* fitted model as a joblib bundle (correct scaler + feature order)\n",
    "\n",
    "B) RANDOM SPLIT (mixed years):\n",
    "   - Train/test split across all 2023‚Äì2026 rows\n",
    "   - Tune GradientBoosting with KFold on TRAIN only\n",
    "   - Evaluate all models on TEST\n",
    "   - Save *every* fitted model as a joblib bundle (correct scaler + feature order)\n",
    "\n",
    "Bundles are saved in:\n",
    "  models_holdout/*.joblib\n",
    "  models_random/*.joblib\n",
    "\n",
    "Each bundle contains:\n",
    "  {\n",
    "    \"tag\": \"...\",\n",
    "    \"model_name\": \"...\",\n",
    "    \"model\": fitted_estimator_or_pipeline,\n",
    "    \"features\": [...],\n",
    "    \"numeric_cols\": [...],\n",
    "    \"binary_cols\": [...],\n",
    "    \"scaler\": fitted_scaler_or_None,\n",
    "    \"metrics\": {\"MAE\":..., \"R2\":...}   # on the chosen evaluation set\n",
    "  }\n",
    "\n",
    "Prediction later:\n",
    "  - Load bundle with joblib.load(...)\n",
    "  - Build X=df[bundle[\"features\"]]\n",
    "  - If bundle[\"scaler\"] not None: scale numeric_cols\n",
    "  - model.predict(X)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GroupKFold, KFold, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# ======================================================\n",
    "# CONFIG\n",
    "# ======================================================\n",
    "TARGET = \"point_spread\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "FEATURES_BASE = [\n",
    "    \"net_rating_diff\",\n",
    "    \"efg_pct_diff\",\n",
    "    \"luck_diff\",\n",
    "    \"adj_off_eff_diff\",\n",
    "    \"home_advantage\",\n",
    "    \"rating_luck_interaction\",\n",
    "    \"shooting_strength_interaction\",\n",
    "    \"is_conference\",\n",
    "    \"is_neutral\",\n",
    "]\n",
    "\n",
    "HOLDOUT_TRAIN_FILES = [\n",
    "    (\"post_info_2023_diff.csv\", 2023),\n",
    "    (\"post_info_2024_diff.csv\", 2024),\n",
    "    (\"post_info_2025_diff.csv\", 2025),\n",
    "]\n",
    "HOLDOUT_TEST_FILE = (\"post_info_2026_diff.csv\", 2026)\n",
    "\n",
    "RANDOM_FILES = [\n",
    "    \"post_info_2023_diff.csv\",\n",
    "    \"post_info_2024_diff.csv\",\n",
    "    \"post_info_2025_diff.csv\",\n",
    "    \"post_info_2026_diff.csv\",\n",
    "]\n",
    "\n",
    "OUT_DIR_HOLDOUT = \"models_holdout\"\n",
    "OUT_DIR_RANDOM  = \"models_random\"\n",
    "\n",
    "os.makedirs(OUT_DIR_HOLDOUT, exist_ok=True)\n",
    "os.makedirs(OUT_DIR_RANDOM,  exist_ok=True)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# HELPERS\n",
    "# ======================================================\n",
    "def infer_cols(features):\n",
    "    \"\"\"\n",
    "    Keep your original rule: don't scale flags, scale everything else.\n",
    "    (Note: home_advantage is treated as \"flag-like\" in your code. We keep that.)\n",
    "    \"\"\"\n",
    "    binary_cols = [c for c in [\"is_conference\", \"is_neutral\", \"home_advantage\"] if c in features]\n",
    "    numeric_cols = [c for c in features if c not in binary_cols]\n",
    "    return numeric_cols, binary_cols\n",
    "\n",
    "\n",
    "def scale_numeric_only(X_train, X_test, numeric_cols):\n",
    "    \"\"\"\n",
    "    Fit scaler on train numeric_cols; transform train/test numeric_cols.\n",
    "    Returns: X_train_scaled, X_test_scaled, fitted_scaler\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = X_train.copy()\n",
    "    Xte = X_test.copy()\n",
    "\n",
    "    if numeric_cols:\n",
    "        Xtr[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "        Xte[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "    return Xtr, Xte, scaler\n",
    "\n",
    "\n",
    "def fit_eval_and_save(\n",
    "    model,\n",
    "    model_name,\n",
    "    X_train_raw,\n",
    "    y_train,\n",
    "    X_test_raw,\n",
    "    y_test,\n",
    "    features,\n",
    "    numeric_cols,\n",
    "    binary_cols,\n",
    "    out_dir,\n",
    "    tag,\n",
    "    *,\n",
    "    needs_external_scaler: bool\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits model and evaluates on test set, then saves a joblib bundle.\n",
    "    - If needs_external_scaler=True: scales numeric_cols using StandardScaler, stores scaler, fits model on scaled.\n",
    "    - If False: fit model directly on raw X (e.g., RandomForest) OR a Pipeline that handles scaling internally.\n",
    "    \"\"\"\n",
    "    m = clone(model)\n",
    "    scaler = None\n",
    "\n",
    "    if needs_external_scaler:\n",
    "        Xtr, Xte, scaler = scale_numeric_only(X_train_raw, X_test_raw, numeric_cols)\n",
    "        m.fit(Xtr, y_train)\n",
    "        pred = m.predict(Xte)\n",
    "    else:\n",
    "        m.fit(X_train_raw, y_train)\n",
    "        pred = m.predict(X_test_raw)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "\n",
    "    bundle = {\n",
    "        \"tag\": tag,\n",
    "        \"model_name\": model_name,\n",
    "        \"model\": m,\n",
    "        \"features\": features,          # exact order\n",
    "        \"numeric_cols\": numeric_cols,\n",
    "        \"binary_cols\": binary_cols,\n",
    "        \"scaler\": scaler,              # None if not used\n",
    "        \"metrics\": {\"MAE\": mae, \"R2\": r2},\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"{model_name}.joblib\")\n",
    "    joblib.dump(bundle, out_path)\n",
    "    print(f\"‚úÖ Saved {tag}: {out_path} | MAE={mae:.4f} R2={r2:.4f}\")\n",
    "\n",
    "    return mae, r2\n",
    "\n",
    "\n",
    "def tune_gb_groupkfold(X_train, y_train, groups, random_state=42):\n",
    "    \"\"\"\n",
    "    Tune GradientBoosting with GroupKFold.\n",
    "    Uses a Pipeline with StandardScaler (harmless).\n",
    "    \"\"\"\n",
    "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "    gb_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", GradientBoostingRegressor(loss=\"absolute_error\", random_state=random_state)),\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": [300, 600, 900, 1200],\n",
    "        \"model__learning_rate\": [0.01, 0.03, 0.05, 0.08],\n",
    "        \"model__max_depth\": [1, 2, 3],\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "        \"model__min_samples_leaf\": [1, 5, 15, 30],\n",
    "    }\n",
    "\n",
    "    cv = GroupKFold(n_splits=3)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        gb_pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring=mae_scorer,\n",
    "        cv=cv,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train, groups=groups)\n",
    "    return search.best_estimator_, search.best_params_\n",
    "\n",
    "\n",
    "def tune_gb_kfold(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Tune GradientBoosting with standard KFold on train only.\n",
    "    Uses a Pipeline with StandardScaler (harmless).\n",
    "    \"\"\"\n",
    "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "    gb_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", GradientBoostingRegressor(loss=\"absolute_error\", random_state=random_state)),\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": [300, 600, 900, 1200],\n",
    "        \"model__learning_rate\": [0.01, 0.03, 0.05, 0.08],\n",
    "        \"model__max_depth\": [1, 2, 3],\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "        \"model__min_samples_leaf\": [1, 5, 15, 30],\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        gb_pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring=mae_scorer,\n",
    "        cv=cv,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    return search.best_estimator_, search.best_params_\n",
    "\n",
    "\n",
    "def build_models(best_gb_tuned):\n",
    "    \"\"\"\n",
    "    Returns dict of model_name -> estimator/pipeline.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(alpha=1.0),\n",
    "        \"Lasso\": Lasso(alpha=0.001, max_iter=10000),\n",
    "        \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000),\n",
    "        \"HuberRegressor\": HuberRegressor(epsilon=1.35, max_iter=2000),\n",
    "        \"RandomForest\": RandomForestRegressor(\n",
    "            n_estimators=500, random_state=RANDOM_STATE, min_samples_leaf=10\n",
    "        ),\n",
    "        \"GradientBoosting_TUNED\": best_gb_tuned,\n",
    "    }\n",
    "\n",
    "\n",
    "def needs_scaler(model_name):\n",
    "    \"\"\"\n",
    "    Match your original behavior:\n",
    "    - linear/huber: scaled numeric only (external scaler)\n",
    "    - RandomForest: raw\n",
    "    - GradientBoosting_TUNED: pipeline handles scaling\n",
    "    \"\"\"\n",
    "    if model_name in [\"RandomForest\", \"GradientBoosting_TUNED\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# A) HOLDOUT: train 2023‚Äì2025, test 2026\n",
    "# ======================================================\n",
    "print(\"\\n================ HOLDOUT (Train 2023‚Äì2025, Test 2026) ================\\n\")\n",
    "\n",
    "# Load holdout train with season groups\n",
    "train_parts = []\n",
    "for f, yr in HOLDOUT_TRAIN_FILES:\n",
    "    d = pd.read_csv(f)\n",
    "    d[\"season\"] = yr\n",
    "    train_parts.append(d)\n",
    "\n",
    "train_df = pd.concat(train_parts, ignore_index=True)\n",
    "test_df = pd.read_csv(HOLDOUT_TEST_FILE[0])\n",
    "\n",
    "# Ensure features exist in BOTH\n",
    "features_holdout = [c for c in FEATURES_BASE if c in train_df.columns and c in test_df.columns]\n",
    "\n",
    "# Drop missing\n",
    "train_df = train_df.dropna(subset=features_holdout + [TARGET]).copy()\n",
    "test_df  = test_df.dropna(subset=features_holdout + [TARGET]).copy()\n",
    "\n",
    "X_train = train_df[features_holdout].copy()\n",
    "y_train = train_df[TARGET].copy()\n",
    "groups  = train_df[\"season\"].copy()\n",
    "\n",
    "X_test = test_df[features_holdout].copy()\n",
    "y_test = test_df[TARGET].copy()\n",
    "\n",
    "numeric_cols, binary_cols = infer_cols(features_holdout)\n",
    "\n",
    "# Tune GB with GroupKFold\n",
    "best_gb, best_params = tune_gb_groupkfold(X_train, y_train, groups, random_state=RANDOM_STATE)\n",
    "print(\"Best params (GB tuned, GroupKFold):\", best_params)\n",
    "\n",
    "# Evaluate and save all models\n",
    "models_holdout = build_models(best_gb)\n",
    "holdout_results = []\n",
    "\n",
    "for name, model in models_holdout.items():\n",
    "    mae, r2 = fit_eval_and_save(\n",
    "        model=model,\n",
    "        model_name=f\"holdout_{name}\",\n",
    "        X_train_raw=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test_raw=X_test,\n",
    "        y_test=y_test,\n",
    "        features=features_holdout,\n",
    "        numeric_cols=numeric_cols,\n",
    "        binary_cols=binary_cols,\n",
    "        out_dir=OUT_DIR_HOLDOUT,\n",
    "        tag=\"HOLDOUT_train_2023_2025\",\n",
    "        needs_external_scaler=needs_scaler(name),\n",
    "    )\n",
    "    holdout_results.append({\"Model\": name, \"MAE\": mae, \"R2\": r2})\n",
    "\n",
    "holdout_res_df = pd.DataFrame(holdout_results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\nüèÄ Holdout Performance Ranking:\")\n",
    "print(holdout_res_df.to_string(index=False))\n",
    "print(\"\\nüåü Best by MAE (holdout):\", holdout_res_df.iloc[0][\"Model\"])\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# B) RANDOM SPLIT: mix all years 2023‚Äì2026\n",
    "# ======================================================\n",
    "print(\"\\n================ RANDOM SPLIT (Mixed 2023‚Äì2026) ================\\n\")\n",
    "\n",
    "df_all = pd.concat([pd.read_csv(f) for f in RANDOM_FILES], ignore_index=True)\n",
    "\n",
    "features_random = [c for c in FEATURES_BASE if c in df_all.columns]\n",
    "df_all = df_all.dropna(subset=features_random + [TARGET]).copy()\n",
    "\n",
    "X = df_all[features_random].copy()\n",
    "y = df_all[TARGET].copy()\n",
    "\n",
    "numeric_cols_r, binary_cols_r = infer_cols(features_random)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Total rows: {len(df_all):,}\")\n",
    "print(f\"‚úÖ Train rows: {len(X_tr):,} | Test rows: {len(X_te):,}\")\n",
    "print(f\"‚úÖ Features ({len(features_random)}): {features_random}\")\n",
    "print(f\"‚úÖ Numeric scaled: {numeric_cols_r}\")\n",
    "print(f\"‚úÖ Left unscaled (flags): {binary_cols_r}\")\n",
    "\n",
    "# Tune GB with KFold on TRAIN only\n",
    "best_gb_r, best_params_r = tune_gb_kfold(X_tr, y_tr, random_state=RANDOM_STATE)\n",
    "print(\"Best params (GB tuned, KFold):\", best_params_r)\n",
    "\n",
    "models_random = build_models(best_gb_r)\n",
    "random_results = []\n",
    "\n",
    "for name, model in models_random.items():\n",
    "    mae, r2 = fit_eval_and_save(\n",
    "        model=model,\n",
    "        model_name=f\"random_{name}\",\n",
    "        X_train_raw=X_tr,\n",
    "        y_train=y_tr,\n",
    "        X_test_raw=X_te,\n",
    "        y_test=y_te,\n",
    "        features=features_random,\n",
    "        numeric_cols=numeric_cols_r,\n",
    "        binary_cols=binary_cols_r,\n",
    "        out_dir=OUT_DIR_RANDOM,\n",
    "        tag=\"RANDOMSPLIT_mixed_2023_2026\",\n",
    "        needs_external_scaler=needs_scaler(name),\n",
    "    )\n",
    "    random_results.append({\"Model\": name, \"MAE\": mae, \"R2\": r2})\n",
    "\n",
    "random_res_df = pd.DataFrame(random_results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\nüèÄ Random-Split Performance Ranking:\")\n",
    "print(random_res_df.to_string(index=False))\n",
    "print(\"\\nüåü Best by MAE (random):\", random_res_df.iloc[0][\"Model\"])\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Done. All models saved into:\")\n",
    "print(f\" - {OUT_DIR_HOLDOUT}/ (holdout bundles)\")\n",
    "print(f\" - {OUT_DIR_RANDOM}/  (random-split bundles)\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# OPTIONAL: HOW TO PREDICT FROM A SAVED BUNDLE (example)\n",
    "# ======================================================\n",
    "# import joblib, pandas as pd\n",
    "#\n",
    "# bundle = joblib.load(\"models_holdout/holdout_GradientBoosting_TUNED.joblib\")\n",
    "# df_future = pd.read_csv(\"post_info_2026_diff.csv\")  # upcoming games feature file\n",
    "#\n",
    "# feats = bundle[\"features\"]\n",
    "# Xf = df_future[feats].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "#\n",
    "# if bundle[\"scaler\"] is not None and bundle[\"numeric_cols\"]:\n",
    "#     Xf[bundle[\"numeric_cols\"]] = bundle[\"scaler\"].transform(Xf[bundle[\"numeric_cols\"]])\n",
    "#\n",
    "# preds = bundle[\"model\"].predict(Xf)\n",
    "# print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ecf9ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ HOLDOUT (Train 2023‚Äì2025, Test 2026) ================\n",
      "\n",
      "Best params (GB tuned, GroupKFold): {'model__subsample': 0.8, 'model__n_estimators': 300, 'model__min_samples_leaf': 15, 'model__max_depth': 3, 'model__learning_rate': 0.03}\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_LinearRegression.joblib | MAE=8.3593 R2=0.7165 | PI q=10.735 cov=70.062% avgW=21.470\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_Ridge.joblib | MAE=8.3597 R2=0.7165 | PI q=10.739 cov=70.062% avgW=21.478\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_Lasso.joblib | MAE=8.3591 R2=0.7165 | PI q=10.740 cov=70.062% avgW=21.480\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_ElasticNet.joblib | MAE=8.3601 R2=0.7164 | PI q=10.731 cov=70.062% avgW=21.462\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_HuberRegressor.joblib | MAE=8.3740 R2=0.7147 | PI q=10.732 cov=70.062% avgW=21.463\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_RandomForest.joblib | MAE=8.5661 R2=0.6993 | PI q=10.994 cov=70.062% avgW=21.989\n",
      "‚úÖ Saved HOLDOUT_train_2023_2025: models_holdout/holdout_GradientBoosting_TUNED.joblib | MAE=8.5742 R2=0.6952 | PI q=11.147 cov=70.062% avgW=22.294\n",
      "\n",
      "üèÄ Holdout Performance Ranking:\n",
      "                 Model      MAE       R2\n",
      "                 Lasso 8.359093 0.716519\n",
      "      LinearRegression 8.359286 0.716507\n",
      "                 Ridge 8.359728 0.716475\n",
      "            ElasticNet 8.360064 0.716449\n",
      "        HuberRegressor 8.374019 0.714744\n",
      "          RandomForest 8.566120 0.699305\n",
      "GradientBoosting_TUNED 8.574240 0.695209\n",
      "\n",
      "üåü Best by MAE (holdout): Lasso\n",
      "\n",
      "================ RANDOM SPLIT (Mixed 2023‚Äì2026) ================\n",
      "\n",
      "‚úÖ Total rows: 7,962\n",
      "‚úÖ Train rows: 6,369 | Test rows: 1,593\n",
      "‚úÖ Features (9): ['net_rating_diff', 'efg_pct_diff', 'luck_diff', 'adj_off_eff_diff', 'home_advantage', 'rating_luck_interaction', 'shooting_strength_interaction', 'is_conference', 'is_neutral']\n",
      "‚úÖ Numeric scaled: ['net_rating_diff', 'efg_pct_diff', 'luck_diff', 'adj_off_eff_diff', 'rating_luck_interaction', 'shooting_strength_interaction']\n",
      "‚úÖ Left unscaled (flags): ['is_conference', 'is_neutral', 'home_advantage']\n",
      "Best params (GB tuned, KFold): {'model__subsample': 0.8, 'model__n_estimators': 600, 'model__min_samples_leaf': 15, 'model__max_depth': 3, 'model__learning_rate': 0.03}\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_LinearRegression.joblib | MAE=8.1016 R2=0.6607 | PI q=10.545 cov=70.119% avgW=21.090\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_Ridge.joblib | MAE=8.1014 R2=0.6607 | PI q=10.551 cov=70.119% avgW=21.101\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_Lasso.joblib | MAE=8.1015 R2=0.6607 | PI q=10.549 cov=70.119% avgW=21.097\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_ElasticNet.joblib | MAE=8.1011 R2=0.6607 | PI q=10.559 cov=70.119% avgW=21.118\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_HuberRegressor.joblib | MAE=8.0803 R2=0.6607 | PI q=10.583 cov=70.119% avgW=21.166\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_RandomForest.joblib | MAE=8.0483 R2=0.6585 | PI q=10.378 cov=70.119% avgW=20.756\n",
      "‚úÖ Saved RANDOMSPLIT_mixed_2023_2026: models_random/random_GradientBoosting_TUNED.joblib | MAE=7.8888 R2=0.6671 | PI q=10.267 cov=70.119% avgW=20.535\n",
      "\n",
      "üèÄ Random-Split Performance Ranking:\n",
      "                 Model      MAE       R2\n",
      "GradientBoosting_TUNED 7.888815 0.667123\n",
      "          RandomForest 8.048282 0.658453\n",
      "        HuberRegressor 8.080316 0.660740\n",
      "            ElasticNet 8.101133 0.660709\n",
      "                 Ridge 8.101432 0.660670\n",
      "                 Lasso 8.101493 0.660667\n",
      "      LinearRegression 8.101564 0.660655\n",
      "\n",
      "üåü Best by MAE (random): GradientBoosting_TUNED\n",
      "\n",
      "‚úÖ Done. All models saved into:\n",
      " - models_holdout/ (holdout bundles)\n",
      " - models_random/  (random-split bundles)\n",
      "\n",
      "================ PREDICT FUTURE GAMES + INTERVALS ================\n",
      "\n",
      "‚úÖ Saved predictions + intervals ‚Üí post_2026_predictions_with_intervals.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TRAIN + SAVE ALL MODELS (HOLDOUT + RANDOM SPLIT) + CALIBRATE CONFORMAL PIs\n",
    "AND (OPTIONAL) GENERATE PREDICTIONS + INTERVALS FOR 2026 FUTURE GAMES.\n",
    "\n",
    "What this script does:\n",
    "\n",
    "A) HOLDOUT (season-based):\n",
    "   - Train on 2023‚Äì2025, test on 2026\n",
    "   - Tune GradientBoosting with GroupKFold (groups=season)\n",
    "   - Evaluate all models on 2026\n",
    "   - Save *every* fitted model as a joblib bundle (correct scaler + feature order)\n",
    "   - Calibrate conformal prediction interval half-width q on the same evaluation set\n",
    "\n",
    "B) RANDOM SPLIT (mixed years):\n",
    "   - Train/test split across all 2023‚Äì2026 rows\n",
    "   - Tune GradientBoosting with KFold on TRAIN only\n",
    "   - Evaluate all models on TEST\n",
    "   - Save *every* fitted model as a joblib bundle (correct scaler + feature order)\n",
    "   - Calibrate conformal prediction interval half-width q on the same evaluation set\n",
    "\n",
    "C) OPTIONAL: Produce a prediction CSV for upcoming games (no y needed)\n",
    "   - Reads FUTURE_FEATURE_FILE\n",
    "   - Adds pred + ci_lb/ci_ub columns for every saved model\n",
    "   - Writes post_2026_predictions_with_intervals.csv\n",
    "\n",
    "Prediction Interval Method:\n",
    "  Split conformal (symmetric):\n",
    "    PI = pred ¬± q\n",
    "  where q is a conservative (1 - alpha) quantile of |residual| on evaluation set.\n",
    "  Default alpha=0.30 => target coverage 70%.\n",
    "\n",
    "Bundles are saved in:\n",
    "  models_holdout/*.joblib\n",
    "  models_random/*.joblib\n",
    "\n",
    "Each bundle contains:\n",
    "  {\n",
    "    \"tag\": \"...\",\n",
    "    \"model_name\": \"...\",\n",
    "    \"model\": fitted_estimator_or_pipeline,\n",
    "    \"features\": [...],\n",
    "    \"numeric_cols\": [...],\n",
    "    \"binary_cols\": [...],\n",
    "    \"scaler\": fitted_scaler_or_None,\n",
    "    \"metrics\": {\"MAE\":..., \"R2\":...},\n",
    "    \"pi_method\": \"split_conformal_abs_residual\",\n",
    "    \"pi_alpha\": 0.30,\n",
    "    \"pi_halfwidth_q\": ...,\n",
    "    \"pi_coverage_eval\": ...,\n",
    "    \"pi_avg_width_eval\": ...\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GroupKFold, KFold, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# CONFIG\n",
    "# ======================================================\n",
    "TARGET = \"point_spread\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "# Prediction interval target coverage = 1 - alpha\n",
    "PI_ALPHA = 0.30  # 70% coverage target\n",
    "\n",
    "FEATURES_BASE = [\n",
    "    \"net_rating_diff\",\n",
    "    \"efg_pct_diff\",\n",
    "    \"luck_diff\",\n",
    "    \"adj_off_eff_diff\",\n",
    "    \"home_advantage\",\n",
    "    \"rating_luck_interaction\",\n",
    "    \"shooting_strength_interaction\",\n",
    "    \"is_conference\",\n",
    "    \"is_neutral\",\n",
    "]\n",
    "\n",
    "HOLDOUT_TRAIN_FILES = [\n",
    "    (\"post_info_2023_diff.csv\", 2023),\n",
    "    (\"post_info_2024_diff.csv\", 2024),\n",
    "    (\"post_info_2025_diff.csv\", 2025),\n",
    "]\n",
    "HOLDOUT_TEST_FILE = (\"post_info_2026_diff.csv\", 2026)\n",
    "\n",
    "RANDOM_FILES = [\n",
    "    \"post_info_2023_diff.csv\",\n",
    "    \"post_info_2024_diff.csv\",\n",
    "    \"post_info_2025_diff.csv\",\n",
    "    \"post_info_2026_diff.csv\",\n",
    "]\n",
    "\n",
    "OUT_DIR_HOLDOUT = \"models_holdout\"\n",
    "OUT_DIR_RANDOM  = \"models_random\"\n",
    "\n",
    "# OPTIONAL: produce predictions+intervals for an upcoming-games feature file.\n",
    "# Set to None to skip. This file MUST contain the engineered feature columns.\n",
    "FUTURE_FEATURE_FILE = \"post_info_2026_diff.csv\"   # change if needed\n",
    "FUTURE_OUT_FILE = \"post_2026_predictions_with_intervals.csv\"\n",
    "\n",
    "os.makedirs(OUT_DIR_HOLDOUT, exist_ok=True)\n",
    "os.makedirs(OUT_DIR_RANDOM,  exist_ok=True)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# HELPERS\n",
    "# ======================================================\n",
    "def infer_cols(features):\n",
    "    \"\"\"\n",
    "    Keep your original rule: don't scale flags, scale everything else.\n",
    "    (home_advantage treated as flag-like)\n",
    "    \"\"\"\n",
    "    binary_cols = [c for c in [\"is_conference\", \"is_neutral\", \"home_advantage\"] if c in features]\n",
    "    numeric_cols = [c for c in features if c not in binary_cols]\n",
    "    return numeric_cols, binary_cols\n",
    "\n",
    "\n",
    "def scale_numeric_only(X_train, X_test, numeric_cols):\n",
    "    \"\"\"\n",
    "    Fit scaler on train numeric_cols; transform train/test numeric_cols.\n",
    "    Returns: X_train_scaled, X_test_scaled, fitted_scaler\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = X_train.copy()\n",
    "    Xte = X_test.copy()\n",
    "\n",
    "    if numeric_cols:\n",
    "        Xtr.loc[:, numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "        Xte.loc[:, numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "    return Xtr, Xte, scaler\n",
    "\n",
    "\n",
    "def conformal_q(y_true, y_pred, alpha=0.30):\n",
    "    \"\"\"\n",
    "    Conservative split-conformal quantile for symmetric PI: pred ¬± q.\n",
    "    q is computed from absolute residuals.\n",
    "    \"\"\"\n",
    "    abs_err = np.abs(np.asarray(y_true) - np.asarray(y_pred))\n",
    "    n = len(abs_err)\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Conservative conformal quantile:\n",
    "    # use ceil((n+1)*(1-alpha))/n and 'higher' method to avoid undercoverage\n",
    "    q_level = np.ceil((n + 1) * (1 - alpha)) / n\n",
    "    q_level = min(max(q_level, 0.0), 1.0)\n",
    "\n",
    "    try:\n",
    "        q = np.quantile(abs_err, q_level, method=\"higher\")\n",
    "    except TypeError:\n",
    "        # older numpy fallback\n",
    "        q = np.quantile(abs_err, q_level, interpolation=\"higher\")\n",
    "\n",
    "    return float(q)\n",
    "\n",
    "\n",
    "def eval_pi(y_true, y_pred, q):\n",
    "    \"\"\"\n",
    "    Evaluate coverage and avg width for symmetric PI pred ¬± q\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    lb = y_pred - q\n",
    "    ub = y_pred + q\n",
    "    coverage = float(np.mean((y_true >= lb) & (y_true <= ub)))\n",
    "    avg_width = float(np.mean(ub - lb))\n",
    "    return coverage, avg_width\n",
    "\n",
    "\n",
    "def fit_eval_and_save(\n",
    "    model,\n",
    "    model_name,\n",
    "    X_train_raw,\n",
    "    y_train,\n",
    "    X_test_raw,\n",
    "    y_test,\n",
    "    features,\n",
    "    numeric_cols,\n",
    "    binary_cols,\n",
    "    out_dir,\n",
    "    tag,\n",
    "    *,\n",
    "    needs_external_scaler: bool,\n",
    "    pi_alpha: float = 0.30,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits model and evaluates on test set, then saves a joblib bundle.\n",
    "    Also calibrates conformal PI half-width q on the same evaluation set.\n",
    "    \"\"\"\n",
    "    m = clone(model)\n",
    "    scaler = None\n",
    "\n",
    "    if needs_external_scaler:\n",
    "        Xtr, Xte, scaler = scale_numeric_only(X_train_raw, X_test_raw, numeric_cols)\n",
    "        m.fit(Xtr, y_train)\n",
    "        pred = m.predict(Xte)\n",
    "    else:\n",
    "        m.fit(X_train_raw, y_train)\n",
    "        pred = m.predict(X_test_raw)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "\n",
    "    # ---- conformal PI calibration on eval set ----\n",
    "    q = conformal_q(y_test, pred, alpha=pi_alpha)\n",
    "    cov, avg_w = eval_pi(y_test, pred, q)\n",
    "\n",
    "    bundle = {\n",
    "        \"tag\": tag,\n",
    "        \"model_name\": model_name,\n",
    "        \"model\": m,\n",
    "        \"features\": features,          # exact order\n",
    "        \"numeric_cols\": numeric_cols,\n",
    "        \"binary_cols\": binary_cols,\n",
    "        \"scaler\": scaler,              # None if not used\n",
    "        \"metrics\": {\"MAE\": float(mae), \"R2\": float(r2)},\n",
    "\n",
    "        \"pi_method\": \"split_conformal_abs_residual\",\n",
    "        \"pi_alpha\": float(pi_alpha),\n",
    "        \"pi_target_coverage\": float(1 - pi_alpha),\n",
    "        \"pi_halfwidth_q\": float(q),\n",
    "        \"pi_coverage_eval\": float(cov),\n",
    "        \"pi_avg_width_eval\": float(avg_w),\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"{model_name}.joblib\")\n",
    "    joblib.dump(bundle, out_path)\n",
    "    print(\n",
    "        f\"‚úÖ Saved {tag}: {out_path} | MAE={mae:.4f} R2={r2:.4f} \"\n",
    "        f\"| PI q={q:.3f} cov={cov:.3%} avgW={avg_w:.3f}\"\n",
    "    )\n",
    "    return mae, r2\n",
    "\n",
    "\n",
    "def tune_gb_groupkfold(X_train, y_train, groups, random_state=42):\n",
    "    \"\"\"\n",
    "    Tune GradientBoosting with GroupKFold (season groups).\n",
    "    Uses a Pipeline with StandardScaler (harmless).\n",
    "    \"\"\"\n",
    "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "    gb_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", GradientBoostingRegressor(loss=\"absolute_error\", random_state=random_state)),\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": [300, 600, 900, 1200],\n",
    "        \"model__learning_rate\": [0.01, 0.03, 0.05, 0.08],\n",
    "        \"model__max_depth\": [1, 2, 3],\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "        \"model__min_samples_leaf\": [1, 5, 15, 30],\n",
    "    }\n",
    "\n",
    "    cv = GroupKFold(n_splits=3)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        gb_pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring=mae_scorer,\n",
    "        cv=cv,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train, groups=groups)\n",
    "    return search.best_estimator_, search.best_params_\n",
    "\n",
    "\n",
    "def tune_gb_kfold(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Tune GradientBoosting with KFold on train only.\n",
    "    Uses a Pipeline with StandardScaler (harmless).\n",
    "    \"\"\"\n",
    "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "    gb_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", GradientBoostingRegressor(loss=\"absolute_error\", random_state=random_state)),\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": [300, 600, 900, 1200],\n",
    "        \"model__learning_rate\": [0.01, 0.03, 0.05, 0.08],\n",
    "        \"model__max_depth\": [1, 2, 3],\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "        \"model__min_samples_leaf\": [1, 5, 15, 30],\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        gb_pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring=mae_scorer,\n",
    "        cv=cv,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    return search.best_estimator_, search.best_params_\n",
    "\n",
    "\n",
    "def build_models(best_gb_tuned):\n",
    "    \"\"\"\n",
    "    Returns dict of model_name -> estimator/pipeline.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(alpha=1.0),\n",
    "        \"Lasso\": Lasso(alpha=0.001, max_iter=10000),\n",
    "        \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000),\n",
    "        \"HuberRegressor\": HuberRegressor(epsilon=1.35, max_iter=2000),\n",
    "        \"RandomForest\": RandomForestRegressor(\n",
    "            n_estimators=500, random_state=RANDOM_STATE, min_samples_leaf=10\n",
    "        ),\n",
    "        \"GradientBoosting_TUNED\": best_gb_tuned,  # Pipeline\n",
    "    }\n",
    "\n",
    "\n",
    "def needs_scaler(model_name):\n",
    "    \"\"\"\n",
    "    - linear/huber: scale numeric only (external scaler)\n",
    "    - RandomForest: raw\n",
    "    - GradientBoosting_TUNED: pipeline handles scaling\n",
    "    \"\"\"\n",
    "    if model_name in [\"RandomForest\", \"GradientBoosting_TUNED\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def bundle_predict_ci(bundle, df_features: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Predict point spread + conformal PI using a saved bundle.\n",
    "    Returns: pred, ci_lb, ci_ub (numpy arrays)\n",
    "    \"\"\"\n",
    "    feats = bundle[\"features\"]\n",
    "    X = df_features[feats].copy()\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    scaler = bundle.get(\"scaler\", None)\n",
    "    numeric_cols = bundle.get(\"numeric_cols\", [])\n",
    "    if scaler is not None and numeric_cols:\n",
    "        X.loc[:, numeric_cols] = scaler.transform(X[numeric_cols])\n",
    "\n",
    "    pred = bundle[\"model\"].predict(X)\n",
    "\n",
    "    q = bundle.get(\"pi_halfwidth_q\", np.nan)\n",
    "    lb = pred - q\n",
    "    ub = pred + q\n",
    "    return pred, lb, ub\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# A) HOLDOUT: train 2023‚Äì2025, test 2026\n",
    "# ======================================================\n",
    "print(\"\\n================ HOLDOUT (Train 2023‚Äì2025, Test 2026) ================\\n\")\n",
    "\n",
    "# Load holdout train with season groups\n",
    "train_parts = []\n",
    "for f, yr in HOLDOUT_TRAIN_FILES:\n",
    "    d = pd.read_csv(f)\n",
    "    d[\"season\"] = yr\n",
    "    train_parts.append(d)\n",
    "\n",
    "train_df = pd.concat(train_parts, ignore_index=True)\n",
    "test_df = pd.read_csv(HOLDOUT_TEST_FILE[0])\n",
    "\n",
    "# Ensure features exist in BOTH\n",
    "features_holdout = [c for c in FEATURES_BASE if c in train_df.columns and c in test_df.columns]\n",
    "\n",
    "# Drop missing\n",
    "train_df = train_df.dropna(subset=features_holdout + [TARGET]).copy()\n",
    "test_df  = test_df.dropna(subset=features_holdout + [TARGET]).copy()\n",
    "\n",
    "X_train = train_df[features_holdout].copy()\n",
    "y_train = train_df[TARGET].copy()\n",
    "groups  = train_df[\"season\"].copy()\n",
    "\n",
    "X_test = test_df[features_holdout].copy()\n",
    "y_test = test_df[TARGET].copy()\n",
    "\n",
    "numeric_cols, binary_cols = infer_cols(features_holdout)\n",
    "\n",
    "# Tune GB with GroupKFold\n",
    "best_gb, best_params = tune_gb_groupkfold(X_train, y_train, groups, random_state=RANDOM_STATE)\n",
    "print(\"Best params (GB tuned, GroupKFold):\", best_params)\n",
    "\n",
    "# Evaluate and save all models\n",
    "models_holdout = build_models(best_gb)\n",
    "holdout_results = []\n",
    "\n",
    "for name, model in models_holdout.items():\n",
    "    mae, r2 = fit_eval_and_save(\n",
    "        model=model,\n",
    "        model_name=f\"holdout_{name}\",\n",
    "        X_train_raw=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test_raw=X_test,\n",
    "        y_test=y_test,\n",
    "        features=features_holdout,\n",
    "        numeric_cols=numeric_cols,\n",
    "        binary_cols=binary_cols,\n",
    "        out_dir=OUT_DIR_HOLDOUT,\n",
    "        tag=\"HOLDOUT_train_2023_2025\",\n",
    "        needs_external_scaler=needs_scaler(name),\n",
    "        pi_alpha=PI_ALPHA,\n",
    "    )\n",
    "    holdout_results.append({\"Model\": name, \"MAE\": mae, \"R2\": r2})\n",
    "\n",
    "holdout_res_df = pd.DataFrame(holdout_results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\nüèÄ Holdout Performance Ranking:\")\n",
    "print(holdout_res_df.to_string(index=False))\n",
    "print(\"\\nüåü Best by MAE (holdout):\", holdout_res_df.iloc[0][\"Model\"])\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# B) RANDOM SPLIT: mix all years 2023‚Äì2026\n",
    "# ======================================================\n",
    "print(\"\\n================ RANDOM SPLIT (Mixed 2023‚Äì2026) ================\\n\")\n",
    "\n",
    "df_all = pd.concat([pd.read_csv(f) for f in RANDOM_FILES], ignore_index=True)\n",
    "\n",
    "features_random = [c for c in FEATURES_BASE if c in df_all.columns]\n",
    "df_all = df_all.dropna(subset=features_random + [TARGET]).copy()\n",
    "\n",
    "X = df_all[features_random].copy()\n",
    "y = df_all[TARGET].copy()\n",
    "\n",
    "numeric_cols_r, binary_cols_r = infer_cols(features_random)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Total rows: {len(df_all):,}\")\n",
    "print(f\"‚úÖ Train rows: {len(X_tr):,} | Test rows: {len(X_te):,}\")\n",
    "print(f\"‚úÖ Features ({len(features_random)}): {features_random}\")\n",
    "print(f\"‚úÖ Numeric scaled: {numeric_cols_r}\")\n",
    "print(f\"‚úÖ Left unscaled (flags): {binary_cols_r}\")\n",
    "\n",
    "# Tune GB with KFold on TRAIN only\n",
    "best_gb_r, best_params_r = tune_gb_kfold(X_tr, y_tr, random_state=RANDOM_STATE)\n",
    "print(\"Best params (GB tuned, KFold):\", best_params_r)\n",
    "\n",
    "models_random = build_models(best_gb_r)\n",
    "random_results = []\n",
    "\n",
    "for name, model in models_random.items():\n",
    "    mae, r2 = fit_eval_and_save(\n",
    "        model=model,\n",
    "        model_name=f\"random_{name}\",\n",
    "        X_train_raw=X_tr,\n",
    "        y_train=y_tr,\n",
    "        X_test_raw=X_te,\n",
    "        y_test=y_te,\n",
    "        features=features_random,\n",
    "        numeric_cols=numeric_cols_r,\n",
    "        binary_cols=binary_cols_r,\n",
    "        out_dir=OUT_DIR_RANDOM,\n",
    "        tag=\"RANDOMSPLIT_mixed_2023_2026\",\n",
    "        needs_external_scaler=needs_scaler(name),\n",
    "        pi_alpha=PI_ALPHA,\n",
    "    )\n",
    "    random_results.append({\"Model\": name, \"MAE\": mae, \"R2\": r2})\n",
    "\n",
    "random_res_df = pd.DataFrame(random_results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\nüèÄ Random-Split Performance Ranking:\")\n",
    "print(random_res_df.to_string(index=False))\n",
    "print(\"\\nüåü Best by MAE (random):\", random_res_df.iloc[0][\"Model\"])\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Done. All models saved into:\")\n",
    "print(f\" - {OUT_DIR_HOLDOUT}/ (holdout bundles)\")\n",
    "print(f\" - {OUT_DIR_RANDOM}/  (random-split bundles)\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# C) OPTIONAL: PREDICT FUTURE GAMES + INTERVALS\n",
    "# ======================================================\n",
    "if FUTURE_FEATURE_FILE is not None and os.path.exists(FUTURE_FEATURE_FILE):\n",
    "    print(\"\\n================ PREDICT FUTURE GAMES + INTERVALS ================\\n\")\n",
    "    future_df = pd.read_csv(FUTURE_FEATURE_FILE)\n",
    "\n",
    "    # You usually want to preserve order + teams\n",
    "    cols_keep = [c for c in [\"home_team\", \"away_team\"] if c in future_df.columns]\n",
    "    out = future_df[cols_keep].copy() if cols_keep else pd.DataFrame(index=future_df.index)\n",
    "\n",
    "    # Predict for every saved model bundle\n",
    "    def add_preds_from_dir(model_dir, prefix):\n",
    "        for fn in sorted(os.listdir(model_dir)):\n",
    "            if not fn.endswith(\".joblib\"):\n",
    "                continue\n",
    "            path = os.path.join(model_dir, fn)\n",
    "            bundle = joblib.load(path)\n",
    "\n",
    "            # Example bundle[\"model_name\"] = \"holdout_Lasso\"\n",
    "            name_clean = bundle[\"model_name\"]\n",
    "            if name_clean.startswith(prefix + \"_\"):\n",
    "                name_clean = name_clean[len(prefix) + 1:]\n",
    "\n",
    "            pred, lb, ub = bundle_predict_ci(bundle, future_df)\n",
    "            out[f\"prediction_spread_{prefix}_{name_clean}\"] = pred\n",
    "            out[f\"ci_lb_{prefix}_{name_clean}\"] = lb\n",
    "            out[f\"ci_ub_{prefix}_{name_clean}\"] = ub\n",
    "\n",
    "    add_preds_from_dir(OUT_DIR_HOLDOUT, \"holdout\")\n",
    "    add_preds_from_dir(OUT_DIR_RANDOM,  \"random\")\n",
    "\n",
    "    # Optional averages (point preds)\n",
    "    holdout_pred_cols = [c for c in out.columns if c.startswith(\"prediction_spread_holdout_\")]\n",
    "    random_pred_cols  = [c for c in out.columns if c.startswith(\"prediction_spread_random_\")]\n",
    "\n",
    "    out[\"avg_holdout\"] = out[holdout_pred_cols].mean(axis=1) if holdout_pred_cols else np.nan\n",
    "    out[\"avg_random\"]  = out[random_pred_cols].mean(axis=1)  if random_pred_cols  else np.nan\n",
    "    out[\"avg_all\"]     = out[holdout_pred_cols + random_pred_cols].mean(axis=1) if (holdout_pred_cols or random_pred_cols) else np.nan\n",
    "\n",
    "    out.to_csv(FUTURE_OUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Saved predictions + intervals ‚Üí {FUTURE_OUT_FILE}\")\n",
    "else:\n",
    "    print(\"\\n(Skipped future predictions: FUTURE_FEATURE_FILE is None or not found.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717fd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
