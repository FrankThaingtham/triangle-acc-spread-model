{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8811b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GLOBAL CONFIG =====\n",
    "import pandas as pd\n",
    "\n",
    "YEAR = 2026\n",
    "\n",
    "# Input files\n",
    "GAME_INFO_FILE = f\"game_info_{YEAR}.csv\"\n",
    "BOX_PATH = f\"game_boxscores_{YEAR}.csv\"\n",
    "\n",
    "# Split outputs (based on dates)\n",
    "PRE_INFO_FILE  = f\"pre_info_{YEAR}.csv\"\n",
    "POST_INFO_FILE = f\"post_info_{YEAR}.csv\"\n",
    "\n",
    "# Team stats\n",
    "TEAM_STATS_FILE         = f\"team_stats_{YEAR}.csv\"\n",
    "TEAM_STATS_UPDATED = f\"team_stats_{YEAR}_updated.csv\"\n",
    "\n",
    "# Post-split with calculated diffs\n",
    "POST_DIFF_FILE = f\"post_info_{YEAR}_diff.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "264c29d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2026 split ‚Üí 4016 pre | 4016 post\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split game_info_<YEAR>.csv into:\n",
    "- pre_<YEAR>.csv  : games from Dec 1 (previous year) to Jan 9\n",
    "- post_<YEAR>.csv : games from Jan 10 to Mar 10 (inclusive)\n",
    "\"\"\"\n",
    "PRE_FILE  = f\"pre_{YEAR}.csv\"\n",
    "POST_FILE = f\"post_{YEAR}.csv\"\n",
    "\n",
    "# Define date boundaries\n",
    "PRE_START  = f\"{int(YEAR)-1}-02-07\"  # December 1 of previous year\n",
    "PRE_END    = f\"{YEAR}-02-06\"\n",
    "POST_START = f\"2025-01-01\"\n",
    "POST_END   = f\"{YEAR}-03-06\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(GAME_INFO_FILE)\n",
    "df[\"game_day\"] = pd.to_datetime(df[\"game_day\"], errors=\"coerce\")\n",
    "\n",
    "# Apply filters\n",
    "pre_df  = df[(df[\"game_day\"] >= PRE_START) & (df[\"game_day\"] <= PRE_END)]\n",
    "post_df = df[(df[\"game_day\"] >= POST_START) & (df[\"game_day\"] <= POST_END)]\n",
    "\n",
    "# Save files\n",
    "pre_df.to_csv(PRE_FILE, index=False)\n",
    "post_df.to_csv(POST_FILE, index=False)\n",
    "\n",
    "print(f\"‚úÖ {YEAR} split ‚Üí {len(pre_df)} pre | {len(post_df)} post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3862e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical teams from PRE: 728\n",
      "Saved match report ‚Üí post_2026_match_report.csv\n",
      "‚úÖ Saved matched post file ‚Üí post_2026_matched.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "YEAR = 2026\n",
    "PRE_FILE  = f\"pre_{YEAR}.csv\"     # your pre window file from CBBpy\n",
    "POST_FILE = f\"post_{YEAR}.csv\"    # your manually created matchups\n",
    "OUT_POST_MATCHED = f\"post_{YEAR}_matched.csv\"\n",
    "OUT_UNMATCHED = f\"post_{YEAR}_unmatched.csv\"\n",
    "OUT_REPORT = f\"post_{YEAR}_match_report.csv\"\n",
    "\n",
    "SCORE_CUTOFF = 90  # raise to be stricter (recommended 88‚Äì95)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"&\", \"and\")\n",
    "    s = s.replace(\".\", \"\")\n",
    "    s = s.replace(\"‚Äô\", \"'\")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "# Optional: manual overrides for very ambiguous short names\n",
    "# IMPORTANT: set the right-side values to EXACT strings that exist in pre_2026.csv\n",
    "OVERRIDES = {\n",
    "    \"Virginia\": \"Virginia Cavaliers\",\n",
    "    \"Miami\": \"Miami Hurricanes\",\n",
    "    \"North Carolina\": \"North Carolina Tar Heels\",\n",
    "    \"Notre Dame\": \"Notre Dame Fighting Irish\",\n",
    "    \"Pitt\": \"Pittsburgh Panthers\",\n",
    "    \"California\": \"California Golden Bears\",\n",
    "    \"Michigan\": \"Michigan Wolverines\",\n",
    "}\n",
    "\n",
    "\n",
    "# --- 1) Build canonical list from PRE ---\n",
    "pre = pd.read_csv(PRE_FILE)\n",
    "\n",
    "canonical = sorted(\n",
    "    set(pre[\"home_team\"].dropna().astype(str)).union(\n",
    "        set(pre[\"away_team\"].dropna().astype(str))\n",
    "    )\n",
    ")\n",
    "\n",
    "# map normalized -> canonical original\n",
    "canon_norm_to_orig = {norm(t): t for t in canonical}\n",
    "canon_norm_list = list(canon_norm_to_orig.keys())\n",
    "\n",
    "print(f\"Canonical teams from PRE: {len(canonical)}\")\n",
    "\n",
    "# --- 2) Load POST and apply overrides first ---\n",
    "post = pd.read_csv(POST_FILE)\n",
    "\n",
    "post[\"home_team\"] = post[\"home_team\"].replace(OVERRIDES)\n",
    "post[\"away_team\"] = post[\"away_team\"].replace(OVERRIDES)\n",
    "\n",
    "# --- 3) Matching function ---\n",
    "def match_to_canonical(name: str):\n",
    "    raw = name\n",
    "    n = norm(raw)\n",
    "\n",
    "    # exact match on normalized form\n",
    "    if n in canon_norm_to_orig:\n",
    "        return canon_norm_to_orig[n], 100\n",
    "\n",
    "    # fuzzy match\n",
    "    best = process.extractOne(n, canon_norm_list, scorer=fuzz.WRatio)\n",
    "    if best is None:\n",
    "        return None, 0\n",
    "\n",
    "    best_norm, score, _ = best\n",
    "    return canon_norm_to_orig[best_norm], score\n",
    "\n",
    "# --- 4) Match all unique teams from POST ---\n",
    "post_teams = sorted(set(post[\"home_team\"].dropna().astype(str)).union(\n",
    "                    set(post[\"away_team\"].dropna().astype(str))))\n",
    "\n",
    "match_rows = []\n",
    "mapping = {}\n",
    "\n",
    "for t in post_teams:\n",
    "    m, score = match_to_canonical(t)\n",
    "    mapping[t] = m\n",
    "    match_rows.append({\n",
    "        \"post_team\": t,\n",
    "        \"matched_pre_team\": m,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "report = pd.DataFrame(match_rows).sort_values([\"score\", \"post_team\"], ascending=[True, True])\n",
    "report.to_csv(OUT_REPORT, index=False)\n",
    "print(f\"Saved match report ‚Üí {OUT_REPORT}\")\n",
    "\n",
    "# --- 5) Apply mapping to POST rows ---\n",
    "post[\"home_team_matched\"] = post[\"home_team\"].map(mapping)\n",
    "post[\"away_team_matched\"] = post[\"away_team\"].map(mapping)\n",
    "\n",
    "# Identify low-confidence / unmatched\n",
    "bad = post[\n",
    "    post[\"home_team_matched\"].isna() |\n",
    "    post[\"away_team_matched\"].isna()\n",
    "].copy()\n",
    "\n",
    "# Also flag low fuzzy scores (below cutoff)\n",
    "low_conf = report[report[\"score\"] < SCORE_CUTOFF]\n",
    "if len(low_conf) > 0:\n",
    "    print(\"Low-confidence matches (review these in the report):\")\n",
    "    print(low_conf)\n",
    "\n",
    "if len(bad) > 0:\n",
    "    bad.to_csv(OUT_UNMATCHED, index=False)\n",
    "    raise ValueError(\n",
    "        f\"Unmatched teams exist in POST. \"\n",
    "        f\"See {OUT_UNMATCHED} and review {OUT_REPORT}.\"\n",
    "    )\n",
    "\n",
    "# Replace with canonical names\n",
    "post[\"home_team\"] = post[\"home_team_matched\"]\n",
    "post[\"away_team\"] = post[\"away_team_matched\"]\n",
    "post = post.drop(columns=[\"home_team_matched\", \"away_team_matched\"])\n",
    "\n",
    "post.to_csv(OUT_POST_MATCHED, index=False)\n",
    "print(f\"‚úÖ Saved matched post file ‚Üí {OUT_POST_MATCHED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "12872ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 21 teams ‚Üí team_stats_2026.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create team_stats_<YEAR>.csv with a single column 'team'\n",
    "containing teams that appear in both pre and post windows.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pre  = pd.read_csv(PRE_FILE)\n",
    "post = pd.read_csv(POST_FILE)\n",
    "\n",
    "teams_pre  = set(pre[\"home_team\"]).union(pre[\"away_team\"])\n",
    "teams_post = set(post[\"home_team\"]).union(post[\"away_team\"])\n",
    "\n",
    "common_teams = sorted(list(teams_pre & teams_post))\n",
    "pd.DataFrame({\"team\": common_teams}).to_csv(TEAM_STATS_FILE, index=False)\n",
    "\n",
    "print(f\"‚úÖ {len(common_teams)} teams ‚Üí {TEAM_STATS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eef072b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added home/away game_id lists ‚Üí team_stats_2026.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Augment team_stats_<YEAR>.csv with columns:\n",
    "- home_game_id: list of pre-season home game_ids\n",
    "- away_game_id: list of pre-season away game_ids\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "pre   = pd.read_csv(PRE_FILE)\n",
    "\n",
    "home_games = pre.groupby(\"home_team\")[\"game_id\"].apply(list).to_dict()\n",
    "away_games = pre.groupby(\"away_team\")[\"game_id\"].apply(list).to_dict()\n",
    "\n",
    "teams[\"home_game_id\"] = teams[\"team\"].map(home_games).apply(lambda x: x if isinstance(x, list) else [])\n",
    "teams[\"away_game_id\"] = teams[\"team\"].map(away_games).apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "teams.to_csv(TEAM_STATS_FILE, index=False)\n",
    "print(f\"‚úÖ Added home/away game_id lists ‚Üí {TEAM_STATS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fa044bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Valid pre-game IDs in team_stats: 349\n",
      "‚úÖ games_dict built for 347 games | skipped: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build nested dictionary for valid games:\n",
    "games_dict[game_id][team_name] = {\n",
    "    \"team_total\": <one-row DataFrame of numeric totals>,\n",
    "    \"player_stats\": <player-level DataFrame for that team in that game>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, ast\n",
    "\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    s = str(x).strip()\n",
    "    if s in (\"\", \"[]\"): return []\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Load boxscores\n",
    "box = pd.read_csv(BOX_PATH)\n",
    "box[\"game_id\"] = pd.to_numeric(box[\"game_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "box = box.dropna(subset=[\"game_id\"]).copy()\n",
    "box[\"game_id\"] = box[\"game_id\"].astype(int)\n",
    "box[\"team\"]    = box[\"team\"].astype(str)\n",
    "\n",
    "# Load teams and collect valid game_ids\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_parse_list).apply(lambda L: [int(i) for i in L])\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_parse_list).apply(lambda L: [int(i) for i in L])\n",
    "\n",
    "valid_ids = set([gid for L in teams[\"home_game_id\"] for gid in L] +\n",
    "                [gid for L in teams[\"away_game_id\"] for gid in L])\n",
    "print(f\"üéØ Valid pre-game IDs in team_stats: {len(valid_ids)}\")\n",
    "\n",
    "# Filter box\n",
    "box = box[box[\"game_id\"].isin(valid_ids)].copy()\n",
    "\n",
    "# Build dict\n",
    "games_dict = {}\n",
    "skipped = 0\n",
    "\n",
    "for gid, gdf in box.groupby(\"game_id\"):\n",
    "    tm = gdf[\"team\"].dropna().unique()\n",
    "    if len(tm) != 2:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    games_dict[gid] = {}\n",
    "    numeric_cols = gdf.select_dtypes(include=\"number\").columns\n",
    "    for t in tm:\n",
    "        df_team = gdf[gdf[\"team\"] == t].copy()\n",
    "        team_total = df_team[numeric_cols].sum(numeric_only=True).to_frame().T.reset_index(drop=True)\n",
    "        games_dict[gid][t] = {\"team_total\": team_total, \"player_stats\": df_team.reset_index(drop=True)}\n",
    "\n",
    "print(f\"‚úÖ games_dict built for {len(games_dict)} games | skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e6186cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaned team_stats: removed 3 invalid game_id refs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove any game_ids in team_stats that are not present in games_dict.\n",
    "Overwrites team_stats_<YEAR>.csv with cleaned lists.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, ast\n",
    "\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_parse_list)\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_parse_list)\n",
    "\n",
    "games_dict_ids = set(map(int, games_dict.keys()))\n",
    "\n",
    "def _keep_known(L): return [int(g) for g in L if int(g) in games_dict_ids]\n",
    "\n",
    "before = teams[\"home_game_id\"].apply(len).sum() + teams[\"away_game_id\"].apply(len).sum()\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_keep_known)\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_keep_known)\n",
    "after  = teams[\"home_game_id\"].apply(len).sum() + teams[\"away_game_id\"].apply(len).sum()\n",
    "\n",
    "teams.to_csv(TEAM_STATS_FILE, index=False)\n",
    "print(f\"üßπ Cleaned team_stats: removed {before - after} invalid game_id refs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "397fa9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Computed per-game eff/shooting for 347 games, 694 team-rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each (game, team) block in games_dict, compute:\n",
    "- poss  = FGA - OREB + TO + 0.475*FTA\n",
    "- off_eff = 100 * PTS / poss\n",
    "- def_eff = opponent's points per opponent poss\n",
    "Also store basic shooting metrics:\n",
    "- eFG% = (FGM + 0.5*3PM) / FGA\n",
    "- TS%  = PTS / [2 * (FGA + 0.44*FTA)]\n",
    "- 3PAr = 3PA / FGA\n",
    "- FTr  = FTA / FGA\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "FT_FACTOR = 0.475\n",
    "\n",
    "def _ensure(team_block, col):\n",
    "    tt = team_block[\"team_total\"]\n",
    "    if col not in tt.columns or pd.isna(tt[col].iloc[0]):\n",
    "        s = pd.to_numeric(team_block[\"player_stats\"].get(col, 0), errors=\"coerce\").fillna(0).sum()\n",
    "        if tt.empty:\n",
    "            team_block[\"team_total\"] = pd.DataFrame([{col: float(s)}])\n",
    "        else:\n",
    "            team_block[\"team_total\"].loc[:, col] = float(s)\n",
    "\n",
    "def _compute_shooting(tt):\n",
    "    # guard zeros\n",
    "    FGA = float(tt.get(\"fga\", [0])[0] if \"fga\" in tt else 0)\n",
    "    FGM = float(tt.get(\"fgm\", [0])[0] if \"fgm\" in tt else 0)\n",
    "    P3M = float(tt.get(\"3pm\", [0])[0] if \"3pm\" in tt else 0)\n",
    "    P3A = float(tt.get(\"3pa\", [0])[0] if \"3pa\" in tt else 0)\n",
    "    FTA = float(tt.get(\"fta\", [0])[0] if \"fta\" in tt else 0)\n",
    "    efg = (FGM + 0.5*P3M) / FGA if FGA > 0 else np.nan\n",
    "    ts  = (tt[\"pts\"].iloc[0]) / (2 * (FGA + 0.44*FTA)) if (FGA + 0.44*FTA) > 0 else np.nan\n",
    "    threepar = P3A / FGA if FGA > 0 else np.nan\n",
    "    ftr  = FTA / FGA if FGA > 0 else np.nan\n",
    "    return efg, ts, threepar, ftr\n",
    "\n",
    "rows = []\n",
    "for gid, teams_block in games_dict.items():\n",
    "    if len(teams_block) != 2: continue\n",
    "    t1, t2 = list(teams_block.keys())\n",
    "\n",
    "    for t in (t1, t2):\n",
    "        for c in [\"fga\",\"oreb\",\"to\",\"fta\",\"pts\",\"fgm\",\"3pm\",\"3pa\"]:\n",
    "            _ensure(teams_block[t], c)\n",
    "\n",
    "    tt1, tt2 = teams_block[t1][\"team_total\"], teams_block[t2][\"team_total\"]\n",
    "\n",
    "    for t_cur, t_opp in [(t1, t2), (t2, t1)]:\n",
    "        cur = teams_block[t_cur][\"team_total\"].copy()\n",
    "        opp = teams_block[t_opp][\"team_total\"].copy()\n",
    "\n",
    "        fga, oreb, tov, fta = [float(pd.to_numeric(cur[c], errors=\"coerce\").iloc[0]) for c in [\"fga\",\"oreb\",\"to\",\"fta\"]]\n",
    "        poss = fga - oreb + tov + FT_FACTOR * fta\n",
    "        pts  = float(pd.to_numeric(cur[\"pts\"], errors=\"coerce\").iloc[0])\n",
    "\n",
    "        poss_opp = float(pd.to_numeric(opp[\"fga\"], errors=\"coerce\").iloc[0] -\n",
    "                         pd.to_numeric(opp[\"oreb\"], errors=\"coerce\").iloc[0] +\n",
    "                         pd.to_numeric(opp[\"to\"], errors=\"coerce\").iloc[0] +\n",
    "                         FT_FACTOR * pd.to_numeric(opp[\"fta\"], errors=\"coerce\").iloc[0])\n",
    "\n",
    "        off_eff = 100.0 * pts / poss if poss > 0 else np.nan\n",
    "        def_eff = 100.0 * float(pd.to_numeric(opp[\"pts\"], errors=\"coerce\").iloc[0]) / poss_opp if poss_opp > 0 else np.nan\n",
    "\n",
    "        efg, ts, threepar, ftr = _compute_shooting(cur)\n",
    "\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"poss\"]    = poss\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"off_eff\"] = off_eff\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"def_eff\"] = def_eff\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"efg_pct\"] = efg\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"ts_pct\"]  = ts\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"threepar\"]= threepar\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"ftr\"]     = ftr\n",
    "\n",
    "        rows.append({\"game_id\": gid, \"team\": t_cur, \"opp\": t_opp,\n",
    "                     \"off_eff\": off_eff, \"def_eff\": def_eff,\n",
    "                     \"efg_pct\": efg, \"ts_pct\": ts, \"threepar\": threepar, \"ftr\": ftr})\n",
    "\n",
    "eff_df = pd.DataFrame(rows)\n",
    "print(f\"‚úÖ Computed per-game eff/shooting for {eff_df['game_id'].nunique()} games, {eff_df.shape[0]} team-rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "84ef73ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved team season averages (raw & adjusted) ‚Üí team_stats_2026_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create per-team season averages and adjusted efficiencies,\n",
    "then save to team_stats_<YEAR>_updated.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Team means\n",
    "team_means = eff_df.groupby(\"team\")[[\"off_eff\",\"def_eff\",\"efg_pct\",\"ts_pct\",\"threepar\",\"ftr\"]].mean()\n",
    "\n",
    "# League means for scaling\n",
    "league_off = team_means[\"off_eff\"].mean()\n",
    "league_def = team_means[\"def_eff\"].mean()\n",
    "\n",
    "# Build opponent averages for adjustment\n",
    "opp_means = eff_df.groupby(\"team\")[[\"off_eff\",\"def_eff\"]].mean().rename(\n",
    "    columns={\"off_eff\":\"avg_off_eff\",\"def_eff\":\"avg_def_eff\"}\n",
    ")\n",
    "\n",
    "eff_adj = eff_df.merge(\n",
    "    opp_means.rename(columns={\"avg_off_eff\":\"avg_off_eff_opp\", \"avg_def_eff\":\"avg_def_eff_opp\"}),\n",
    "    left_on=\"opp\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "eff_adj[\"adj_off_eff\"] = (eff_adj[\"off_eff\"] / eff_adj[\"avg_def_eff_opp\"]) * league_off\n",
    "eff_adj[\"adj_def_eff\"] = (eff_adj[\"def_eff\"] / eff_adj[\"avg_off_eff_opp\"]) * league_def\n",
    "\n",
    "# Season averages (adjusted + raw shooting)\n",
    "team_adj_means = eff_adj.groupby(\"team\")[[\"adj_off_eff\",\"adj_def_eff\"]].mean()\n",
    "team_stats_means = team_means.join(team_adj_means, how=\"left\")\n",
    "\n",
    "# Merge into team_stats file\n",
    "base = pd.read_csv(TEAM_STATS_FILE)\n",
    "team_stats_updated = base.merge(\n",
    "    team_stats_means.reset_index(),\n",
    "    on=\"team\", how=\"left\"\n",
    ")\n",
    "\n",
    "team_stats_updated.to_csv(TEAM_STATS_UPDATED, index=False)\n",
    "print(f\"‚úÖ Saved team season averages (raw & adjusted) ‚Üí {TEAM_STATS_UPDATED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7470a82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid game_ids in team_stats: 347\n",
      "Original pre rows: 4016\n",
      "Filtered pre rows: 347\n",
      "Saved: pre_2026_filtered_by_team_stats_game_ids.csv\n",
      "Valid IDs not found in pre_ file: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def _parse_list_cell(x):\n",
    "    \"\"\"Parse a cell that may be a list, NaN, or a string like '[1, 2, 3]'.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if s in (\"\", \"[]\"):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        # fallback: try splitting by comma\n",
    "        s = s.strip(\"[]\")\n",
    "        if not s:\n",
    "            return []\n",
    "        return [i.strip() for i in s.split(\",\") if i.strip()]\n",
    "\n",
    "def filter_pre_by_team_stats_game_ids(\n",
    "    pre_df: pd.DataFrame,\n",
    "    team_stats_df: pd.DataFrame,\n",
    "    *,\n",
    "    pre_game_id_col: str = \"game_id\",\n",
    "    home_ids_col: str = \"home_game_id\",\n",
    "    away_ids_col: str = \"away_game_id\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep only rows in pre_df where game_id is present in any team's home_game_id or away_game_id lists.\n",
    "    Returns (filtered_df, valid_ids_set).\n",
    "    \"\"\"\n",
    "    # parse lists\n",
    "    home_lists = team_stats_df[home_ids_col].apply(_parse_list_cell)\n",
    "    away_lists = team_stats_df[away_ids_col].apply(_parse_list_cell)\n",
    "\n",
    "    # flatten -> set of ints\n",
    "    valid_ids = set()\n",
    "    for L in pd.concat([home_lists, away_lists], ignore_index=True):\n",
    "        for gid in L:\n",
    "            try:\n",
    "                valid_ids.add(int(gid))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # normalize pre game_id\n",
    "    pre_ids = pd.to_numeric(pre_df[pre_game_id_col], errors=\"coerce\").dropna().astype(int)\n",
    "\n",
    "    filtered = pre_df.loc[pre_ids.isin(valid_ids)].copy()\n",
    "    return filtered, valid_ids\n",
    "\n",
    "# ---- Example usage ----\n",
    "YEAR = 2026\n",
    "PRE_FILE = f\"pre_{YEAR}.csv\"\n",
    "TEAM_STATS_FILE = f\"team_stats_{YEAR}_updated.csv\"  # or team_stats_{YEAR}.csv if that's where lists are\n",
    "\n",
    "pre = pd.read_csv(PRE_FILE)\n",
    "team_stats = pd.read_csv(TEAM_STATS_FILE)\n",
    "\n",
    "pre_filtered, valid_ids = filter_pre_by_team_stats_game_ids(pre, team_stats)\n",
    "\n",
    "out = f\"pre_{YEAR}_filtered_by_team_stats_game_ids.csv\"\n",
    "pre_filtered.to_csv(out, index=False)\n",
    "\n",
    "print(\"Valid game_ids in team_stats:\", len(valid_ids))\n",
    "print(\"Original pre rows:\", len(pre))\n",
    "print(\"Filtered pre rows:\", len(pre_filtered))\n",
    "print(\"Saved:\", out)\n",
    "\n",
    "# Optional diagnostics: which valid IDs are missing from pre?\n",
    "pre_game_ids_set = set(pd.to_numeric(pre[\"game_id\"], errors=\"coerce\").dropna().astype(int))\n",
    "missing_in_pre = sorted(list(valid_ids - pre_game_ids_set))\n",
    "print(\"Valid IDs not found in pre_ file:\", len(missing_in_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea6cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "70883fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Computed luck from pre_2026.csv and merged into team_stats_2026_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute team luck from pre_2026.csv and merge into team_stats_2026_updated.csv.\n",
    "\n",
    "luck = actual_win_pct - expected_win_pct\n",
    "expected_win_pct computed from avg_margin via logistic transform.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "YEAR = 2026\n",
    "PRE_CSV = f\"pre_{YEAR}.csv\"                  # <-- explicitly re-read pre_2026.csv\n",
    "TEAM_STATS_UPDATED = f\"team_stats_{YEAR}_updated.csv\"\n",
    "\n",
    "pre = pd.read_csv(PRE_CSV)\n",
    "\n",
    "# Ensure numeric scores and keep only games with scores\n",
    "pre[\"home_score\"] = pd.to_numeric(pre[\"home_score\"], errors=\"coerce\")\n",
    "pre[\"away_score\"] = pd.to_numeric(pre[\"away_score\"], errors=\"coerce\")\n",
    "pre = pre.dropna(subset=[\"home_score\", \"away_score\"]).copy()\n",
    "\n",
    "# Margin + win\n",
    "pre[\"point_spread\"] = pre[\"home_score\"] - pre[\"away_score\"]\n",
    "pre[\"home_win\"] = (pre[\"point_spread\"] > 0).astype(int)\n",
    "\n",
    "# Home-team view\n",
    "home_df = pre[[\"home_team\", \"point_spread\", \"home_win\"]].copy()\n",
    "home_df[\"team\"] = home_df[\"home_team\"]\n",
    "home_df[\"margin\"] = home_df[\"point_spread\"]\n",
    "home_df[\"win\"] = home_df[\"home_win\"]\n",
    "\n",
    "# Away-team view\n",
    "away_df = pre[[\"away_team\", \"point_spread\", \"home_win\"]].copy()\n",
    "away_df[\"team\"] = away_df[\"away_team\"]\n",
    "away_df[\"margin\"] = -away_df[\"point_spread\"]\n",
    "away_df[\"win\"] = 1 - away_df[\"home_win\"]\n",
    "\n",
    "team_games = pd.concat(\n",
    "    [home_df[[\"team\", \"margin\", \"win\"]], away_df[[\"team\", \"margin\", \"win\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "summary = (team_games.groupby(\"team\")\n",
    "           .agg(actual_win_pct=(\"win\", \"mean\"),\n",
    "                avg_margin=(\"margin\", \"mean\"))\n",
    "           .reset_index())\n",
    "\n",
    "summary[\"expected_win_pct\"] = 1 / (1 + 10 ** (-summary[\"avg_margin\"] / 10))\n",
    "summary[\"luck\"] = summary[\"actual_win_pct\"] - summary[\"expected_win_pct\"]\n",
    "\n",
    "luck = summary[[\"team\", \"luck\"]]\n",
    "\n",
    "# Merge into team_stats_2026_updated.csv\n",
    "stats = pd.read_csv(TEAM_STATS_UPDATED)\n",
    "\n",
    "# Replace old luck if present\n",
    "if \"luck\" in stats.columns:\n",
    "    stats = stats.drop(columns=[\"luck\"])\n",
    "\n",
    "stats = stats.merge(luck, on=\"team\", how=\"left\")\n",
    "stats.to_csv(TEAM_STATS_UPDATED, index=False)\n",
    "\n",
    "print(f\"‚úÖ Computed luck from {PRE_CSV} and merged into {TEAM_STATS_UPDATED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26d1ea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enriched post_2026.csv with season stats, net ratings, and luck\n"
     ]
    }
   ],
   "source": [
    "# Build home/away versions of season stats you want on POST_FILE\n",
    "home_cols = {\n",
    "    \"team\":\"home_team\",\n",
    "    \"off_eff\":\"off_eff_hometeam\",\n",
    "    \"def_eff\":\"def_eff_hometeam\",\n",
    "    \"adj_off_eff\":\"adj_off_eff_hometeam\",\n",
    "    \"adj_def_eff\":\"adj_def_eff_hometeam\",\n",
    "    \"efg_pct\":\"efg_pct_hometeam\",\n",
    "    \"ts_pct\":\"ts_pct_hometeam\",\n",
    "    \"threepar\":\"threepar_hometeam\",\n",
    "    \"ftr\":\"ftr_hometeam\"\n",
    "}\n",
    "away_cols = {k: v.replace(\"home\",\"away\") for k, v in home_cols.items()}\n",
    "\n",
    "post_enriched = post.merge(stats.rename(columns=home_cols), on=\"home_team\", how=\"left\")\n",
    "post_enriched = post_enriched.merge(stats.rename(columns=away_cols), on=\"away_team\", how=\"left\")\n",
    "\n",
    "# Net ratings\n",
    "post_enriched[\"net_rating_hometeam\"] = post_enriched[\"adj_off_eff_hometeam\"] - post_enriched[\"adj_def_eff_hometeam\"]\n",
    "post_enriched[\"net_rating_awayteam\"] = post_enriched[\"adj_off_eff_awayteam\"] - post_enriched[\"adj_def_eff_awayteam\"]\n",
    "\n",
    "# Luck (home/away)\n",
    "post_enriched = post_enriched.merge(luck.rename(columns={\"team\":\"home_team\",\"luck\":\"luck_hometeam\"}), on=\"home_team\", how=\"left\")\n",
    "post_enriched = post_enriched.merge(luck.rename(columns={\"team\":\"away_team\",\"luck\":\"luck_awayteam\"}), on=\"away_team\", how=\"left\")\n",
    "\n",
    "post_enriched.to_csv(POST_FILE, index=False)\n",
    "print(f\"‚úÖ Enriched {POST_FILE} with season stats, net ratings, and luck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "902b9bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added home_advantage_score + home_advantage ‚Üí team_stats_2026_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pre = pd.read_csv(PRE_FILE)\n",
    "\n",
    "# ensure numeric\n",
    "pre[\"home_score\"] = pd.to_numeric(pre[\"home_score\"], errors=\"coerce\")\n",
    "pre[\"away_score\"] = pd.to_numeric(pre[\"away_score\"], errors=\"coerce\")\n",
    "pre = pre.dropna(subset=[\"home_score\",\"away_score\"]).copy()\n",
    "\n",
    "if \"is_neutral\" in pre.columns:\n",
    "    pre = pre[pre[\"is_neutral\"] == False].copy()\n",
    "\n",
    "pre[\"home_margin\"] = pre[\"home_score\"] - pre[\"away_score\"]\n",
    "pre[\"away_margin\"] = -pre[\"home_margin\"]\n",
    "\n",
    "home_avg = (pre.groupby(\"home_team\")[\"home_margin\"]\n",
    "            .mean().reset_index()\n",
    "            .rename(columns={\"home_team\":\"team\",\"home_margin\":\"avg_home_margin\"}))\n",
    "\n",
    "away_avg = (pre.groupby(\"away_team\")[\"away_margin\"]\n",
    "            .mean().reset_index()\n",
    "            .rename(columns={\"away_team\":\"team\",\"away_margin\":\"avg_away_margin\"}))\n",
    "\n",
    "adv = pd.merge(home_avg, away_avg, on=\"team\", how=\"outer\").fillna(0.0)\n",
    "adv[\"home_advantage_score\"] = adv[\"avg_home_margin\"] - adv[\"avg_away_margin\"]\n",
    "\n",
    "team_stats_updated = pd.read_csv(TEAM_STATS_UPDATED)\n",
    "\n",
    "# avoid duplicate merge columns if rerun\n",
    "for c in [\"home_advantage_score\", \"home_advantage\"]:\n",
    "    if c in team_stats_updated.columns:\n",
    "        team_stats_updated = team_stats_updated.drop(columns=[c])\n",
    "\n",
    "team_stats_updated = team_stats_updated.merge(\n",
    "    adv[[\"team\", \"home_advantage_score\"]],\n",
    "    on=\"team\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Alias column for compatibility\n",
    "team_stats_updated[\"home_advantage\"] = team_stats_updated[\"home_advantage_score\"]\n",
    "\n",
    "if {\"adj_off_eff\",\"adj_def_eff\"}.issubset(team_stats_updated.columns):\n",
    "    team_stats_updated[\"net_rating\"] = team_stats_updated[\"adj_off_eff\"] - team_stats_updated[\"adj_def_eff\"]\n",
    "\n",
    "team_stats_updated.to_csv(TEAM_STATS_UPDATED, index=False)\n",
    "print(f\"‚úÖ Added home_advantage_score + home_advantage ‚Üí {TEAM_STATS_UPDATED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae597fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv columns: ['team', 'avg_home_margin', 'avg_away_margin', 'home_advantage_score', 'home_advantage']\n",
      "‚úÖ Added home_advantage_score/home_advantage ‚Üí team_stats_2026_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pre = pd.read_csv(PRE_FILE)\n",
    "\n",
    "# Ensure numeric scores\n",
    "pre[\"home_score\"] = pd.to_numeric(pre[\"home_score\"], errors=\"coerce\")\n",
    "pre[\"away_score\"] = pd.to_numeric(pre[\"away_score\"], errors=\"coerce\")\n",
    "pre = pre.dropna(subset=[\"home_score\", \"away_score\"]).copy()\n",
    "\n",
    "# Neutral filter (handle bool/string)\n",
    "if \"is_neutral\" in pre.columns:\n",
    "    is_neutral = pre[\"is_neutral\"]\n",
    "    if is_neutral.dtype == object:\n",
    "        is_neutral = is_neutral.astype(str).str.strip().str.lower().map({\"true\": True, \"false\": False})\n",
    "    pre = pre[is_neutral == False].copy()\n",
    "\n",
    "pre[\"home_margin\"] = pre[\"home_score\"] - pre[\"away_score\"]\n",
    "pre[\"away_margin\"] = -pre[\"home_margin\"]\n",
    "\n",
    "home_avg = (pre.groupby(\"home_team\")[\"home_margin\"]\n",
    "            .mean().reset_index()\n",
    "            .rename(columns={\"home_team\": \"team\", \"home_margin\": \"avg_home_margin\"}))\n",
    "\n",
    "away_avg = (pre.groupby(\"away_team\")[\"away_margin\"]\n",
    "            .mean().reset_index()\n",
    "            .rename(columns={\"away_team\": \"team\", \"away_margin\": \"avg_away_margin\"}))\n",
    "\n",
    "adv = pd.merge(home_avg, away_avg, on=\"team\", how=\"outer\")\n",
    "\n",
    "# Fill only the numeric columns, not the team name\n",
    "adv[\"avg_home_margin\"] = adv[\"avg_home_margin\"].fillna(0.0)\n",
    "adv[\"avg_away_margin\"] = adv[\"avg_away_margin\"].fillna(0.0)\n",
    "\n",
    "# Create the column you need (and a compatibility alias)\n",
    "adv[\"home_advantage_score\"] = adv[\"avg_home_margin\"] - adv[\"avg_away_margin\"]\n",
    "adv[\"home_advantage\"] = adv[\"home_advantage_score\"]\n",
    "\n",
    "print(\"adv columns:\", adv.columns.tolist())  # should include home_advantage_score\n",
    "\n",
    "team_stats_updated = pd.read_csv(TEAM_STATS_UPDATED)\n",
    "\n",
    "# Avoid duplicate columns if rerun\n",
    "for c in [\"home_advantage_score\", \"home_advantage\"]:\n",
    "    if c in team_stats_updated.columns:\n",
    "        team_stats_updated = team_stats_updated.drop(columns=[c])\n",
    "\n",
    "team_stats_updated = team_stats_updated.merge(\n",
    "    adv[[\"team\", \"home_advantage_score\", \"home_advantage\"]],\n",
    "    on=\"team\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "if {\"adj_off_eff\", \"adj_def_eff\"}.issubset(team_stats_updated.columns):\n",
    "    team_stats_updated[\"net_rating\"] = team_stats_updated[\"adj_off_eff\"] - team_stats_updated[\"adj_def_eff\"]\n",
    "\n",
    "team_stats_updated.to_csv(TEAM_STATS_UPDATED, index=False)\n",
    "print(f\"‚úÖ Added home_advantage_score/home_advantage ‚Üí {TEAM_STATS_UPDATED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4f4991f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added home_advantage to post_2026.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "post = pd.read_csv(POST_FILE)\n",
    "stats = pd.read_csv(TEAM_STATS_UPDATED)  # has team + home_advantage_score\n",
    "\n",
    "# ensure the column exists in team stats\n",
    "if \"home_advantage_score\" not in stats.columns:\n",
    "    raise ValueError(\"home_advantage_score not found in TEAM_STATS_UPDATED. Add it there first.\")\n",
    "\n",
    "# merge onto home team and name it exactly what your model expects\n",
    "post = post.merge(\n",
    "    stats[[\"team\", \"home_advantage_score\"]].rename(\n",
    "        columns={\"team\": \"home_team\", \"home_advantage_score\": \"home_advantage\"}\n",
    "    ),\n",
    "    on=\"home_team\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "post.to_csv(POST_FILE, index=False)\n",
    "print(f\"‚úÖ Added home_advantage to {POST_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c38f8161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved diff dataset ‚Üí post_info_2026_diff.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute home - away differences for key features and save post_<YEAR>_diff.csv.\n",
    "Keeps core game info + diff columns + (optional) home_advantage.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(POST_FILE)\n",
    "\n",
    "# Ensure booleans are numeric\n",
    "for c in [\"is_conference\",\"is_neutral\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "# Differences\n",
    "pairs = [\n",
    "    (\"off_eff_hometeam\", \"off_eff_awayteam\"),\n",
    "    (\"def_eff_hometeam\", \"def_eff_awayteam\"),\n",
    "    (\"adj_off_eff_hometeam\", \"adj_off_eff_awayteam\"),\n",
    "    (\"adj_def_eff_hometeam\", \"adj_def_eff_awayteam\"),\n",
    "    (\"efg_pct_hometeam\", \"efg_pct_awayteam\"),\n",
    "    (\"ts_pct_hometeam\", \"ts_pct_awayteam\"),\n",
    "    (\"threepar_hometeam\", \"threepar_awayteam\"),\n",
    "    (\"ftr_hometeam\", \"ftr_awayteam\"),\n",
    "    (\"luck_hometeam\", \"luck_awayteam\"),\n",
    "    (\"net_rating_hometeam\",\"net_rating_awayteam\"),\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    if a in df.columns and b in df.columns:\n",
    "        df[a.replace(\"_hometeam\",\"_diff\")] = df[a] - df[b]\n",
    "\n",
    "# Select columns to keep\n",
    "keep = [\n",
    "    \"home_team\",\"away_team\",\"home_advantage\"  # optional if computed\n",
    "] + [c for c in df.columns if c.endswith(\"_diff\")]\n",
    "\n",
    "df = df[keep]\n",
    "df.to_csv(POST_DIFF_FILE, index=False)\n",
    "print(f\"‚úÖ Saved diff dataset ‚Üí {POST_DIFF_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a6706b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added interaction terms ‚Üí post_info_2026_diff.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optional nonlinearity: add interaction terms to post_<YEAR>_diff.csv\n",
    "- rating_luck_interaction = net_rating_diff * luck_diff\n",
    "- shooting_strength_interaction = net_rating_diff * efg_pct_diff\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(POST_DIFF_FILE)\n",
    "for req in [\"net_rating_diff\",\"luck_diff\",\"efg_pct_diff\"]:\n",
    "    if req not in df.columns:\n",
    "        raise ValueError(f\"Missing {req} in {POST_DIFF_FILE}\")\n",
    "\n",
    "df[\"rating_luck_interaction\"] = df[\"net_rating_diff\"] * df[\"luck_diff\"]\n",
    "df[\"shooting_strength_interaction\"] = df[\"net_rating_diff\"] * df[\"efg_pct_diff\"]\n",
    "df.to_csv(POST_DIFF_FILE, index=False)\n",
    "print(f\"‚úÖ Added interaction terms ‚Üí {POST_DIFF_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01e02233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using 9 features: ['net_rating_diff', 'efg_pct_diff', 'luck_diff', 'adj_off_eff_diff', 'home_advantage', 'rating_luck_interaction', 'shooting_strength_interaction', 'is_conference', 'is_neutral']\n",
      "‚úÖ Train size: 2803 | Test size: 1202\n"
     ]
    }
   ],
   "source": [
    "# === 1) SETUP & SPLIT ================================================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load diff dataset from your global POST_DIFF_FILE\n",
    "df = pd.read_csv(POST_DIFF_FILE)\n",
    "\n",
    "# Choose features & target\n",
    "features = [\n",
    "    \"net_rating_diff\",\n",
    "    \"efg_pct_diff\",\n",
    "    \"luck_diff\",\n",
    "    \"adj_off_eff_diff\",\n",
    "    \"home_advantage\",               \n",
    "    \"rating_luck_interaction\",      \n",
    "    \"shooting_strength_interaction\",\n",
    "    \"is_conference\",\n",
    "    \"is_neutral\",\n",
    "]\n",
    "# Keep only columns that actually exist\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "TARGET = \"point_spread\"\n",
    "df = df.dropna(subset=features + [TARGET])\n",
    "\n",
    "# Split features/target\n",
    "X = df[features]\n",
    "y = df[TARGET]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Scale (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Using {len(features)} features: {features}\")\n",
    "print(f\"‚úÖ Train size: {X_train.shape[0]} | Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "095a363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÄ Model Performance Comparison:\n",
      "           Model      MAE       R¬≤\n",
      "  HuberRegressor 8.324063 0.722645\n",
      "GradientBoosting 8.332656 0.723186\n",
      "           Lasso 8.338055 0.722503\n",
      "      ElasticNet 8.338110 0.722542\n",
      "           Ridge 8.338242 0.722521\n",
      "LinearRegression 8.338285 0.722487\n",
      "    RandomForest 8.436191 0.714263\n",
      "\n",
      "üåü Best by MAE: HuberRegressor\n"
     ]
    }
   ],
   "source": [
    "# === 2) MODEL TRAINING ===============================================\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.001, max_iter=10000),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000),\n",
    "    \"HuberRegressor\": HuberRegressor(epsilon=1.35, max_iter=1000),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=42),\n",
    "}\n",
    "\n",
    "results, trained = [], {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained[name] = model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"R¬≤\": r2_score(y_test, y_pred),\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\nüèÄ Model Performance Comparison:\")\n",
    "print(res.to_string(index=False))\n",
    "\n",
    "# Keep the best model name for analysis chunk\n",
    "best_model_name = res.iloc[0][\"Model\"]\n",
    "print(f\"\\nüåü Best by MAE: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3915435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Feature correlation matrix (train set):\n",
      "                               net_rating_diff  efg_pct_diff  luck_diff  \\\n",
      "net_rating_diff                          1.000         0.689     -0.398   \n",
      "efg_pct_diff                             0.689         1.000     -0.333   \n",
      "luck_diff                               -0.398        -0.333      1.000   \n",
      "adj_off_eff_diff                         0.825         0.829     -0.391   \n",
      "home_advantage                           0.684         0.526     -0.281   \n",
      "rating_luck_interaction                  0.272         0.186     -0.116   \n",
      "shooting_strength_interaction           -0.379        -0.309      0.107   \n",
      "is_conference                              NaN           NaN        NaN   \n",
      "is_neutral                                 NaN           NaN        NaN   \n",
      "\n",
      "                               adj_off_eff_diff  home_advantage  \\\n",
      "net_rating_diff                           0.825           0.684   \n",
      "efg_pct_diff                              0.829           0.526   \n",
      "luck_diff                                -0.391          -0.281   \n",
      "adj_off_eff_diff                          1.000           0.593   \n",
      "home_advantage                            0.593           1.000   \n",
      "rating_luck_interaction                   0.234           0.217   \n",
      "shooting_strength_interaction            -0.348          -0.322   \n",
      "is_conference                               NaN             NaN   \n",
      "is_neutral                                  NaN             NaN   \n",
      "\n",
      "                               rating_luck_interaction  \\\n",
      "net_rating_diff                                  0.272   \n",
      "efg_pct_diff                                     0.186   \n",
      "luck_diff                                       -0.116   \n",
      "adj_off_eff_diff                                 0.234   \n",
      "home_advantage                                   0.217   \n",
      "rating_luck_interaction                          1.000   \n",
      "shooting_strength_interaction                   -0.547   \n",
      "is_conference                                      NaN   \n",
      "is_neutral                                         NaN   \n",
      "\n",
      "                               shooting_strength_interaction  is_conference  \\\n",
      "net_rating_diff                                       -0.379            NaN   \n",
      "efg_pct_diff                                          -0.309            NaN   \n",
      "luck_diff                                              0.107            NaN   \n",
      "adj_off_eff_diff                                      -0.348            NaN   \n",
      "home_advantage                                        -0.322            NaN   \n",
      "rating_luck_interaction                               -0.547            NaN   \n",
      "shooting_strength_interaction                          1.000            NaN   \n",
      "is_conference                                            NaN            NaN   \n",
      "is_neutral                                               NaN            NaN   \n",
      "\n",
      "                               is_neutral  \n",
      "net_rating_diff                       NaN  \n",
      "efg_pct_diff                          NaN  \n",
      "luck_diff                             NaN  \n",
      "adj_off_eff_diff                      NaN  \n",
      "home_advantage                        NaN  \n",
      "rating_luck_interaction               NaN  \n",
      "shooting_strength_interaction         NaN  \n",
      "is_conference                         NaN  \n",
      "is_neutral                            NaN  \n",
      "\n",
      "üîé Top coefficients (LinearRegression) on standardized features:\n",
      "net_rating_diff                  8.173542\n",
      "luck_diff                       -1.504184\n",
      "rating_luck_interaction          1.484759\n",
      "home_advantage                   0.481329\n",
      "adj_off_eff_diff                 0.308677\n",
      "efg_pct_diff                    -0.224800\n",
      "shooting_strength_interaction    0.104747\n",
      "is_conference                    0.000000\n",
      "is_neutral                       0.000000\n",
      "\n",
      "üîé Top coefficients (Ridge) on standardized features:\n",
      "net_rating_diff                  8.139287\n",
      "luck_diff                       -1.504995\n",
      "rating_luck_interaction          1.483510\n",
      "home_advantage                   0.490536\n",
      "adj_off_eff_diff                 0.329000\n",
      "efg_pct_diff                    -0.223685\n",
      "shooting_strength_interaction    0.101441\n",
      "is_conference                    0.000000\n",
      "is_neutral                       0.000000\n",
      "\n",
      "üîé Top coefficients (Lasso) on standardized features:\n",
      "net_rating_diff                  8.173365\n",
      "luck_diff                       -1.503624\n",
      "rating_luck_interaction          1.482861\n",
      "home_advantage                   0.480271\n",
      "adj_off_eff_diff                 0.303447\n",
      "efg_pct_diff                    -0.219034\n",
      "shooting_strength_interaction    0.102205\n",
      "is_conference                    0.000000\n",
      "is_neutral                       0.000000\n",
      "\n",
      "üîé Top coefficients (ElasticNet) on standardized features:\n",
      "net_rating_diff                  8.158238\n",
      "luck_diff                       -1.504262\n",
      "rating_luck_interaction          1.483258\n",
      "home_advantage                   0.484891\n",
      "adj_off_eff_diff                 0.315127\n",
      "efg_pct_diff                    -0.221460\n",
      "shooting_strength_interaction    0.102011\n",
      "is_conference                    0.000000\n",
      "is_neutral                       0.000000\n",
      "\n",
      "üîé Top coefficients (HuberRegressor) on standardized features:\n",
      "net_rating_diff                  7.886660\n",
      "rating_luck_interaction          1.984303\n",
      "luck_diff                       -0.824597\n",
      "home_advantage                   0.643119\n",
      "shooting_strength_interaction    0.452407\n",
      "efg_pct_diff                     0.083877\n",
      "adj_off_eff_diff                 0.082974\n",
      "is_conference                    0.000000\n",
      "is_neutral                       0.000000\n",
      "\n",
      "üå≤ Feature importances (RandomForest):\n",
      "net_rating_diff                  0.484420\n",
      "rating_luck_interaction          0.132384\n",
      "adj_off_eff_diff                 0.088154\n",
      "home_advantage                   0.083859\n",
      "luck_diff                        0.079872\n",
      "efg_pct_diff                     0.068178\n",
      "shooting_strength_interaction    0.063133\n",
      "is_conference                    0.000000\n",
      "is_neutral                       0.000000\n",
      "\n",
      "üå≤ Feature importances (GradientBoosting):\n",
      "net_rating_diff                  0.585246\n",
      "rating_luck_interaction          0.106222\n",
      "adj_off_eff_diff                 0.087898\n",
      "luck_diff                        0.066281\n",
      "efg_pct_diff                     0.063650\n",
      "home_advantage                   0.054209\n",
      "shooting_strength_interaction    0.036494\n",
      "is_conference                    0.000000\n",
      "is_neutral                       0.000000\n",
      "\n",
      "üéØ Permutation importance (ŒîMAE when shuffled) for HuberRegressor:\n",
      "home_advantage                   0.004456\n",
      "efg_pct_diff                     0.002763\n",
      "adj_off_eff_diff                 0.002406\n",
      "is_conference                   -0.000000\n",
      "is_neutral                      -0.000000\n",
      "luck_diff                       -0.007122\n",
      "shooting_strength_interaction   -0.018062\n",
      "rating_luck_interaction         -0.218575\n",
      "net_rating_diff                 -3.279009\n"
     ]
    }
   ],
   "source": [
    "# === 3) ANALYSIS ======================================================\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nüìå Feature correlation matrix (train set):\")\n",
    "corr = pd.DataFrame(X_train, columns=features).corr()\n",
    "print(corr.round(3))\n",
    "\n",
    "# Coefficients for linear-type models (on standardized features)\n",
    "def show_linear_coefs(name):\n",
    "    if name in trained and hasattr(trained[name], \"coef_\"):\n",
    "        coefs = pd.Series(trained[name].coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "        print(f\"\\nüîé Top coefficients ({name}) on standardized features:\")\n",
    "        print(coefs.to_string())\n",
    "    else:\n",
    "        print(f\"\\n({name}) has no linear coefficients to display.\")\n",
    "\n",
    "for lin in [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"HuberRegressor\"]:\n",
    "    show_linear_coefs(lin)\n",
    "\n",
    "# Tree-based importance (if available)\n",
    "def show_importance(name):\n",
    "    if name in trained and hasattr(trained[name], \"feature_importances_\"):\n",
    "        imps = pd.Series(trained[name].feature_importances_, index=features).sort_values(ascending=False)\n",
    "        print(f\"\\nüå≤ Feature importances ({name}):\")\n",
    "        print(imps.to_string())\n",
    "    else:\n",
    "        print(f\"\\n({name}) has no tree feature_importances_ to display.\")\n",
    "\n",
    "for tree in [\"RandomForest\", \"GradientBoosting\"]:\n",
    "    show_importance(tree)\n",
    "\n",
    "# Optional: permutation importance for the best model (more robust)\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    best_model = trained[best_model_name]\n",
    "    perm = permutation_importance(best_model, X_test_scaled, y_test, scoring=\"neg_mean_absolute_error\",\n",
    "                                  n_repeats=10, random_state=42)\n",
    "    perm_imps = pd.Series(-perm.importances_mean, index=features).sort_values(ascending=False)\n",
    "    print(f\"\\nüéØ Permutation importance (ŒîMAE when shuffled) for {best_model_name}:\")\n",
    "    print(perm_imps.to_string())\n",
    "except Exception as e:\n",
    "    print(f\"\\n(perm importance skipped) {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "875da4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Learning Curve (MAE ¬± sd)\n",
      "Size   Best_MAE  ¬±sd     Ridge_MAE  ¬±sd\n",
      " 10%      8.038 ¬±0.472        7.977 ¬±0.346\n",
      " 20%      7.583 ¬±0.073        7.625 ¬±0.069\n",
      " 30%      7.534 ¬±0.088        7.624 ¬±0.110\n",
      " 40%      7.452 ¬±0.071        7.537 ¬±0.073\n",
      " 50%      7.455 ¬±0.055        7.530 ¬±0.069\n",
      " 60%      7.425 ¬±0.020        7.506 ¬±0.029\n",
      " 70%      7.411 ¬±0.013        7.492 ¬±0.022\n",
      " 80%      7.410 ¬±0.028        7.493 ¬±0.034\n",
      " 90%      7.407 ¬±0.018        7.493 ¬±0.018\n",
      "100%      7.396 ¬±0.000        7.483 ¬±0.000\n",
      "\n",
      "üß≠ Diagnosis:\n",
      "- Best model improvement 50%‚Üí100%: 0.059 MAE\n",
      "- Ridge improvement 50%‚Üí100%     : 0.047 MAE\n",
      "- Last-step improvement (best)    : 0.011 MAE\n",
      "\n",
      "‚úÖ Verdict: üß™ Curve is flat: focus on richer features (tempo/recent form/rest/travel/etc.).\n"
     ]
    }
   ],
   "source": [
    "# === 2.5) LEARNING CURVE (plug-and-play) ==============================\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import clone\n",
    "\n",
    "def learning_curve_mae(model, Xtr, ytr, Xte, yte, sizes=None, repeats=8, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains `model` on increasing fractions of (Xtr, ytr) and evaluates MAE on (Xte, yte).\n",
    "    Returns sizes, mean MAE per size, and std MAE per size.\n",
    "    \"\"\"\n",
    "    if sizes is None:\n",
    "        sizes = np.linspace(0.1, 1.0, 10)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    means, stds = [], []\n",
    "    ytr_np = ytr.to_numpy() if hasattr(ytr, \"to_numpy\") else np.asarray(ytr)\n",
    "    for s in sizes:\n",
    "        n = max(10, int(len(Xtr) * s))\n",
    "        maes = []\n",
    "        for _ in range(repeats):\n",
    "            idx = rng.choice(len(Xtr), n, replace=False)\n",
    "            m = clone(model)\n",
    "            m.fit(Xtr[idx], ytr_np[idx])\n",
    "            pred = m.predict(Xte)\n",
    "            maes.append(mean_absolute_error(yte, pred))\n",
    "        means.append(float(np.mean(maes)))\n",
    "        stds.append(float(np.std(maes)))\n",
    "    return np.array(sizes), np.array(means), np.array(stds)\n",
    "\n",
    "# Pick the best model from your leaderboard (already computed above)\n",
    "best_model = trained[best_model_name]\n",
    "\n",
    "# Run learning curves for the best model and a Ridge baseline\n",
    "sizes, best_mean, best_std = learning_curve_mae(best_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "_,     ridge_mean, ridge_std = learning_curve_mae(Ridge(alpha=1.0), X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Pretty print\n",
    "print(\"\\nüìà Learning Curve (MAE ¬± sd)\")\n",
    "print(\"Size   Best_MAE  ¬±sd     Ridge_MAE  ¬±sd\")\n",
    "for s, bm, bs, rm, rs in zip(sizes, best_mean, best_std, ridge_mean, ridge_std):\n",
    "    print(f\"{int(s*100):>3d}%   {bm:8.3f} ¬±{bs:4.3f}   {rm:10.3f} ¬±{rs:4.3f}\")\n",
    "\n",
    "# Heuristic verdict\n",
    "def improvement(means, sizes_arr):\n",
    "    # improvement from ~50% to 100%\n",
    "    mid_idx = np.argmin(np.abs(sizes_arr - 0.5))\n",
    "    return means[mid_idx] - means[-1]\n",
    "\n",
    "best_improve = improvement(best_mean, sizes)\n",
    "ridge_improve = improvement(ridge_mean, sizes)\n",
    "tail_slope = best_mean[-2] - best_mean[-1] if len(best_mean) > 1 else 0.0\n",
    "\n",
    "print(\"\\nüß≠ Diagnosis:\")\n",
    "print(f\"- Best model improvement 50%‚Üí100%: {best_improve:.3f} MAE\")\n",
    "print(f\"- Ridge improvement 50%‚Üí100%     : {ridge_improve:.3f} MAE\")\n",
    "print(f\"- Last-step improvement (best)    : {tail_slope:.3f} MAE\")\n",
    "\n",
    "if best_improve >= 0.30 or tail_slope > 0.05:\n",
    "    verdict = \"üì¶ More data likely helps (curve still descending).\"\n",
    "elif best_improve < 0.10 and abs(tail_slope) < 0.02:\n",
    "    verdict = \"üß™ Curve is flat: focus on richer features (tempo/recent form/rest/travel/etc.).\"\n",
    "else:\n",
    "    verdict = \"‚öñÔ∏è Mixed: modest data gains possible, but new features will likely matter more.\"\n",
    "\n",
    "print(f\"\\n‚úÖ Verdict: {verdict}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
