{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdede78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GLOBAL CONFIG =====\n",
    "import pandas as pd\n",
    "\n",
    "YEAR = 2023\n",
    "\n",
    "# Input files\n",
    "GAME_INFO_FILE = f\"game_info_{YEAR}.csv\"\n",
    "BOX_PATH = f\"game_boxscores_{YEAR}.csv\"\n",
    "\n",
    "# Split outputs (based on dates)\n",
    "PRE_INFO_FILE  = f\"pre_info_{YEAR}.csv\"\n",
    "POST_INFO_FILE = f\"post_info_{YEAR}.csv\"\n",
    "\n",
    "# Team stats\n",
    "TEAM_STATS_FILE         = f\"team_stats_{YEAR}.csv\"\n",
    "TEAM_STATS_UPDATED = f\"team_stats_{YEAR}_updated.csv\"\n",
    "\n",
    "# Post-split with calculated diffs\n",
    "POST_DIFF_FILE = f\"post_info_{YEAR}_diff.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "313d4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 2023 split â†’ 996 pre | 1241 post\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split game_info_<YEAR>.csv into:\n",
    "- pre_<YEAR>.csv  : games from Dec 1 (previous year) to Jan 9\n",
    "- post_<YEAR>.csv : games from Jan 10 to Mar 10 (inclusive)\n",
    "\"\"\"\n",
    "PRE_FILE  = f\"pre_{YEAR}.csv\"\n",
    "POST_FILE = f\"post_{YEAR}.csv\"\n",
    "\n",
    "# Define date boundaries\n",
    "PRE_START  = f\"{int(YEAR)-1}-02-07\"  # December 1 of previous year\n",
    "PRE_END    = f\"{YEAR}-02-06\"\n",
    "POST_START = f\"2022-11-07\"\n",
    "POST_END   = f\"{YEAR}-03-06\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(GAME_INFO_FILE)\n",
    "df[\"game_day\"] = pd.to_datetime(df[\"game_day\"], errors=\"coerce\")\n",
    "\n",
    "# Apply filters\n",
    "pre_df  = df[(df[\"game_day\"] >= PRE_START) & (df[\"game_day\"] <= PRE_END)]\n",
    "post_df = df[(df[\"game_day\"] >= POST_START) & (df[\"game_day\"] <= POST_END)]\n",
    "\n",
    "# Save files\n",
    "pre_df.to_csv(PRE_FILE, index=False)\n",
    "post_df.to_csv(POST_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… {YEAR} split â†’ {len(pre_df)} pre | {len(post_df)} post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48ebeb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 306 teams â†’ team_stats_2023.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create team_stats_<YEAR>.csv with a single column 'team'\n",
    "containing teams that appear in both pre and post windows.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pre  = pd.read_csv(PRE_FILE)\n",
    "post = pd.read_csv(POST_FILE)\n",
    "\n",
    "teams_pre  = set(pre[\"home_team\"]).union(pre[\"away_team\"])\n",
    "teams_post = set(post[\"home_team\"]).union(post[\"away_team\"])\n",
    "\n",
    "common_teams = sorted(list(teams_pre & teams_post))\n",
    "pd.DataFrame({\"team\": common_teams}).to_csv(TEAM_STATS_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… {len(common_teams)} teams â†’ {TEAM_STATS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5607be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added home/away game_id lists â†’ team_stats_2023.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Augment team_stats_<YEAR>.csv with columns:\n",
    "- home_game_id: list of pre-season home game_ids\n",
    "- away_game_id: list of pre-season away game_ids\n",
    "\"\"\"\n",
    "\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "pre   = pd.read_csv(PRE_FILE)\n",
    "\n",
    "home_games = pre.groupby(\"home_team\")[\"game_id\"].apply(list).to_dict()\n",
    "away_games = pre.groupby(\"away_team\")[\"game_id\"].apply(list).to_dict()\n",
    "\n",
    "teams[\"home_game_id\"] = teams[\"team\"].map(home_games).apply(lambda x: x if isinstance(x, list) else [])\n",
    "teams[\"away_game_id\"] = teams[\"team\"].map(away_games).apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "teams.to_csv(TEAM_STATS_FILE, index=False)\n",
    "print(f\"âœ… Added home/away game_id lists â†’ {TEAM_STATS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26da0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Valid pre-game IDs in team_stats: 996\n",
      "âœ… games_dict built for 996 games | skipped: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build nested dictionary for valid games:\n",
    "games_dict[game_id][team_name] = {\n",
    "    \"team_total\": <one-row DataFrame of numeric totals>,\n",
    "    \"player_stats\": <player-level DataFrame for that team in that game>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, ast\n",
    "\n",
    "def _parse_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    s = str(x).strip()\n",
    "    if s in (\"\", \"[]\"): return []\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Load boxscores\n",
    "box = pd.read_csv(BOX_PATH)\n",
    "box[\"game_id\"] = pd.to_numeric(box[\"game_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "box = box.dropna(subset=[\"game_id\"]).copy()\n",
    "box[\"game_id\"] = box[\"game_id\"].astype(int)\n",
    "box[\"team\"]    = box[\"team\"].astype(str)\n",
    "\n",
    "# Load teams and collect valid game_ids\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_parse_list).apply(lambda L: [int(i) for i in L])\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_parse_list).apply(lambda L: [int(i) for i in L])\n",
    "\n",
    "valid_ids = set([gid for L in teams[\"home_game_id\"] for gid in L] +\n",
    "                [gid for L in teams[\"away_game_id\"] for gid in L])\n",
    "print(f\"ðŸŽ¯ Valid pre-game IDs in team_stats: {len(valid_ids)}\")\n",
    "\n",
    "# Filter box\n",
    "box = box[box[\"game_id\"].isin(valid_ids)].copy()\n",
    "\n",
    "# Build dict\n",
    "games_dict = {}\n",
    "skipped = 0\n",
    "\n",
    "for gid, gdf in box.groupby(\"game_id\"):\n",
    "    tm = gdf[\"team\"].dropna().unique()\n",
    "    if len(tm) != 2:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    games_dict[gid] = {}\n",
    "    numeric_cols = gdf.select_dtypes(include=\"number\").columns\n",
    "    for t in tm:\n",
    "        df_team = gdf[gdf[\"team\"] == t].copy()\n",
    "        team_total = df_team[numeric_cols].sum(numeric_only=True).to_frame().T.reset_index(drop=True)\n",
    "        games_dict[gid][t] = {\"team_total\": team_total, \"player_stats\": df_team.reset_index(drop=True)}\n",
    "\n",
    "print(f\"âœ… games_dict built for {len(games_dict)} games | skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0ed4edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaned team_stats: removed 0 invalid game_id refs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Remove any game_ids in team_stats that are not present in games_dict.\n",
    "Overwrites team_stats_<YEAR>.csv with cleaned lists.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, ast\n",
    "\n",
    "teams = pd.read_csv(TEAM_STATS_FILE)\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_parse_list)\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_parse_list)\n",
    "\n",
    "games_dict_ids = set(map(int, games_dict.keys()))\n",
    "\n",
    "def _keep_known(L): return [int(g) for g in L if int(g) in games_dict_ids]\n",
    "\n",
    "before = teams[\"home_game_id\"].apply(len).sum() + teams[\"away_game_id\"].apply(len).sum()\n",
    "teams[\"home_game_id\"] = teams[\"home_game_id\"].apply(_keep_known)\n",
    "teams[\"away_game_id\"] = teams[\"away_game_id\"].apply(_keep_known)\n",
    "after  = teams[\"home_game_id\"].apply(len).sum() + teams[\"away_game_id\"].apply(len).sum()\n",
    "\n",
    "teams.to_csv(TEAM_STATS_FILE, index=False)\n",
    "print(f\"ðŸ§¹ Cleaned team_stats: removed {before - after} invalid game_id refs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2eb0d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Computed per-game eff/shooting for 996 games, 1992 team-rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For each (game, team) block in games_dict, compute:\n",
    "- poss  = FGA - OREB + TO + 0.475*FTA\n",
    "- off_eff = 100 * PTS / poss\n",
    "- def_eff = opponent's points per opponent poss\n",
    "Also store basic shooting metrics:\n",
    "- eFG% = (FGM + 0.5*3PM) / FGA\n",
    "- TS%  = PTS / [2 * (FGA + 0.44*FTA)]\n",
    "- 3PAr = 3PA / FGA\n",
    "- FTr  = FTA / FGA\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "FT_FACTOR = 0.475\n",
    "\n",
    "def _ensure(team_block, col):\n",
    "    tt = team_block[\"team_total\"]\n",
    "    if col not in tt.columns or pd.isna(tt[col].iloc[0]):\n",
    "        s = pd.to_numeric(team_block[\"player_stats\"].get(col, 0), errors=\"coerce\").fillna(0).sum()\n",
    "        if tt.empty:\n",
    "            team_block[\"team_total\"] = pd.DataFrame([{col: float(s)}])\n",
    "        else:\n",
    "            team_block[\"team_total\"].loc[:, col] = float(s)\n",
    "\n",
    "def _compute_shooting(tt):\n",
    "    # guard zeros\n",
    "    FGA = float(tt.get(\"fga\", [0])[0] if \"fga\" in tt else 0)\n",
    "    FGM = float(tt.get(\"fgm\", [0])[0] if \"fgm\" in tt else 0)\n",
    "    P3M = float(tt.get(\"3pm\", [0])[0] if \"3pm\" in tt else 0)\n",
    "    P3A = float(tt.get(\"3pa\", [0])[0] if \"3pa\" in tt else 0)\n",
    "    FTA = float(tt.get(\"fta\", [0])[0] if \"fta\" in tt else 0)\n",
    "    efg = (FGM + 0.5*P3M) / FGA if FGA > 0 else np.nan\n",
    "    ts  = (tt[\"pts\"].iloc[0]) / (2 * (FGA + 0.44*FTA)) if (FGA + 0.44*FTA) > 0 else np.nan\n",
    "    threepar = P3A / FGA if FGA > 0 else np.nan\n",
    "    ftr  = FTA / FGA if FGA > 0 else np.nan\n",
    "    return efg, ts, threepar, ftr\n",
    "\n",
    "rows = []\n",
    "for gid, teams_block in games_dict.items():\n",
    "    if len(teams_block) != 2: continue\n",
    "    t1, t2 = list(teams_block.keys())\n",
    "\n",
    "    for t in (t1, t2):\n",
    "        for c in [\"fga\",\"oreb\",\"to\",\"fta\",\"pts\",\"fgm\",\"3pm\",\"3pa\"]:\n",
    "            _ensure(teams_block[t], c)\n",
    "\n",
    "    tt1, tt2 = teams_block[t1][\"team_total\"], teams_block[t2][\"team_total\"]\n",
    "\n",
    "    for t_cur, t_opp in [(t1, t2), (t2, t1)]:\n",
    "        cur = teams_block[t_cur][\"team_total\"].copy()\n",
    "        opp = teams_block[t_opp][\"team_total\"].copy()\n",
    "\n",
    "        fga, oreb, tov, fta = [float(pd.to_numeric(cur[c], errors=\"coerce\").iloc[0]) for c in [\"fga\",\"oreb\",\"to\",\"fta\"]]\n",
    "        poss = fga - oreb + tov + FT_FACTOR * fta\n",
    "        pts  = float(pd.to_numeric(cur[\"pts\"], errors=\"coerce\").iloc[0])\n",
    "\n",
    "        poss_opp = float(pd.to_numeric(opp[\"fga\"], errors=\"coerce\").iloc[0] -\n",
    "                         pd.to_numeric(opp[\"oreb\"], errors=\"coerce\").iloc[0] +\n",
    "                         pd.to_numeric(opp[\"to\"], errors=\"coerce\").iloc[0] +\n",
    "                         FT_FACTOR * pd.to_numeric(opp[\"fta\"], errors=\"coerce\").iloc[0])\n",
    "\n",
    "        off_eff = 100.0 * pts / poss if poss > 0 else np.nan\n",
    "        def_eff = 100.0 * float(pd.to_numeric(opp[\"pts\"], errors=\"coerce\").iloc[0]) / poss_opp if poss_opp > 0 else np.nan\n",
    "\n",
    "        efg, ts, threepar, ftr = _compute_shooting(cur)\n",
    "\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"poss\"]    = poss\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"off_eff\"] = off_eff\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"def_eff\"] = def_eff\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"efg_pct\"] = efg\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"ts_pct\"]  = ts\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"threepar\"]= threepar\n",
    "        games_dict[gid][t_cur][\"team_total\"].loc[:, \"ftr\"]     = ftr\n",
    "\n",
    "        rows.append({\"game_id\": gid, \"team\": t_cur, \"opp\": t_opp,\n",
    "                     \"off_eff\": off_eff, \"def_eff\": def_eff,\n",
    "                     \"efg_pct\": efg, \"ts_pct\": ts, \"threepar\": threepar, \"ftr\": ftr})\n",
    "\n",
    "eff_df = pd.DataFrame(rows)\n",
    "print(f\"âœ… Computed per-game eff/shooting for {eff_df['game_id'].nunique()} games, {eff_df.shape[0]} team-rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db84498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved team season averages (raw & adjusted) â†’ team_stats_2023_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create per-team season averages and adjusted efficiencies,\n",
    "then save to team_stats_<YEAR>_updated.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Team means\n",
    "team_means = eff_df.groupby(\"team\")[[\"off_eff\",\"def_eff\",\"efg_pct\",\"ts_pct\",\"threepar\",\"ftr\"]].mean()\n",
    "\n",
    "# League means for scaling\n",
    "league_off = team_means[\"off_eff\"].mean()\n",
    "league_def = team_means[\"def_eff\"].mean()\n",
    "\n",
    "# Build opponent averages for adjustment\n",
    "opp_means = eff_df.groupby(\"team\")[[\"off_eff\",\"def_eff\"]].mean().rename(\n",
    "    columns={\"off_eff\":\"avg_off_eff\",\"def_eff\":\"avg_def_eff\"}\n",
    ")\n",
    "\n",
    "eff_adj = eff_df.merge(\n",
    "    opp_means.rename(columns={\"avg_off_eff\":\"avg_off_eff_opp\", \"avg_def_eff\":\"avg_def_eff_opp\"}),\n",
    "    left_on=\"opp\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "eff_adj[\"adj_off_eff\"] = (eff_adj[\"off_eff\"] / eff_adj[\"avg_def_eff_opp\"]) * league_off\n",
    "eff_adj[\"adj_def_eff\"] = (eff_adj[\"def_eff\"] / eff_adj[\"avg_off_eff_opp\"]) * league_def\n",
    "\n",
    "# Season averages (adjusted + raw shooting)\n",
    "team_adj_means = eff_adj.groupby(\"team\")[[\"adj_off_eff\",\"adj_def_eff\"]].mean()\n",
    "team_stats_means = team_means.join(team_adj_means, how=\"left\")\n",
    "\n",
    "# Merge into team_stats file\n",
    "base = pd.read_csv(TEAM_STATS_FILE)\n",
    "team_stats_updated = base.merge(\n",
    "    team_stats_means.reset_index(),\n",
    "    on=\"team\", how=\"left\"\n",
    ")\n",
    "\n",
    "team_stats_updated.to_csv(TEAM_STATS_UPDATED, index=False)\n",
    "print(f\"âœ… Saved team season averages (raw & adjusted) â†’ {TEAM_STATS_UPDATED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08a447b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched post_2023.csv with season stats, net ratings, and luck\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute team luck from post_<YEAR>.csv and merge (luck_hometeam / luck_awayteam)\n",
    "Then also compute net_rating for each side.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "post = pd.read_csv(POST_FILE)\n",
    "\n",
    "# point spread & booleans (ensure numeric)\n",
    "post[\"point_spread\"] = post[\"home_score\"] - post[\"away_score\"]\n",
    "\n",
    "# Build per-team win/margin from post window\n",
    "post[\"home_win\"] = (post[\"point_spread\"] > 0).astype(int)\n",
    "\n",
    "home_df = post.copy()\n",
    "home_df[\"team\"] = home_df[\"home_team\"]\n",
    "home_df[\"margin\"] = home_df[\"point_spread\"]\n",
    "home_df[\"win\"] = home_df[\"home_win\"]\n",
    "\n",
    "away_df = post.copy()\n",
    "away_df[\"team\"] = away_df[\"away_team\"]\n",
    "away_df[\"margin\"] = -away_df[\"point_spread\"]\n",
    "away_df[\"win\"] = 1 - away_df[\"home_win\"]\n",
    "\n",
    "team_games = pd.concat([home_df, away_df], ignore_index=True)\n",
    "\n",
    "summary = (team_games.groupby(\"team\")\n",
    "           .agg(actual_win_pct=(\"win\",\"mean\"),\n",
    "                avg_margin=(\"margin\",\"mean\"))\n",
    "           .reset_index())\n",
    "\n",
    "summary[\"expected_win_pct\"] = 1 / (1 + 10 ** (-summary[\"avg_margin\"] / 10))\n",
    "summary[\"luck\"] = summary[\"actual_win_pct\"] - summary[\"expected_win_pct\"]\n",
    "\n",
    "luck = summary[[\"team\",\"luck\"]]\n",
    "\n",
    "# Merge season efficiencies from updated team file\n",
    "stats = pd.read_csv(TEAM_STATS_UPDATED)\n",
    "\n",
    "# Build home/away versions of season stats you want on POST_FILE\n",
    "home_cols = {\n",
    "    \"team\":\"home_team\",\n",
    "    \"off_eff\":\"off_eff_hometeam\",\n",
    "    \"def_eff\":\"def_eff_hometeam\",\n",
    "    \"adj_off_eff\":\"adj_off_eff_hometeam\",\n",
    "    \"adj_def_eff\":\"adj_def_eff_hometeam\",\n",
    "    \"efg_pct\":\"efg_pct_hometeam\",\n",
    "    \"ts_pct\":\"ts_pct_hometeam\",\n",
    "    \"threepar\":\"threepar_hometeam\",\n",
    "    \"ftr\":\"ftr_hometeam\"\n",
    "}\n",
    "away_cols = {k: v.replace(\"home\",\"away\") for k, v in home_cols.items()}\n",
    "\n",
    "post_enriched = post.merge(stats.rename(columns=home_cols), on=\"home_team\", how=\"left\")\n",
    "post_enriched = post_enriched.merge(stats.rename(columns=away_cols), on=\"away_team\", how=\"left\")\n",
    "\n",
    "# Net ratings\n",
    "post_enriched[\"net_rating_hometeam\"] = post_enriched[\"adj_off_eff_hometeam\"] - post_enriched[\"adj_def_eff_hometeam\"]\n",
    "post_enriched[\"net_rating_awayteam\"] = post_enriched[\"adj_off_eff_awayteam\"] - post_enriched[\"adj_def_eff_awayteam\"]\n",
    "\n",
    "# Luck (home/away)\n",
    "post_enriched = post_enriched.merge(luck.rename(columns={\"team\":\"home_team\",\"luck\":\"luck_hometeam\"}), on=\"home_team\", how=\"left\")\n",
    "post_enriched = post_enriched.merge(luck.rename(columns={\"team\":\"away_team\",\"luck\":\"luck_awayteam\"}), on=\"away_team\", how=\"left\")\n",
    "\n",
    "post_enriched.to_csv(POST_FILE, index=False)\n",
    "print(f\"âœ… Enriched {POST_FILE} with season stats, net ratings, and luck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3447e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added 'home_advantage' to POST_FILE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute home_advantage_score per team from pre window and add to POST and POST_DIFF later.\n",
    "home_advantage_score = avg_home_margin - avg_away_margin (excluding neutral)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pre = pd.read_csv(PRE_FILE)\n",
    "if \"is_neutral\" in pre.columns:\n",
    "    pre = pre[pre[\"is_neutral\"] == False]\n",
    "\n",
    "pre[\"home_margin\"] = pre[\"home_score\"] - pre[\"away_score\"]\n",
    "\n",
    "home_avg = pre.groupby(\"home_team\")[\"home_margin\"].mean().reset_index().rename(\n",
    "    columns={\"home_team\":\"team\",\"home_margin\":\"avg_home_margin\"}\n",
    ")\n",
    "away_avg = pre.groupby(\"away_team\")[\"home_margin\"].mean().reset_index().rename(\n",
    "    columns={\"away_team\":\"team\",\"home_margin\":\"avg_away_margin\"}\n",
    ")\n",
    "\n",
    "adv = pd.merge(home_avg, away_avg, on=\"team\", how=\"outer\").fillna(0)\n",
    "adv[\"home_advantage_score\"] = adv[\"avg_home_margin\"] - adv[\"avg_away_margin\"]\n",
    "\n",
    "# Merge only the home team's advantage into POST_FILE\n",
    "post = pd.read_csv(POST_FILE)\n",
    "post = post.merge(adv[[\"team\",\"home_advantage_score\"]].rename(columns={\"team\":\"home_team\",\"home_advantage_score\":\"home_advantage\"}),\n",
    "                  on=\"home_team\", how=\"left\")\n",
    "post.to_csv(POST_FILE, index=False)\n",
    "print(\"âœ… Added 'home_advantage' to POST_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf77494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved diff dataset â†’ post_info_2023_diff.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute home - away differences for key features and save post_<YEAR>_diff.csv.\n",
    "Keeps core game info + diff columns + (optional) home_advantage.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(POST_FILE)\n",
    "\n",
    "# Ensure booleans are numeric\n",
    "for c in [\"is_conference\",\"is_neutral\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "# Differences\n",
    "pairs = [\n",
    "    (\"off_eff_hometeam\", \"off_eff_awayteam\"),\n",
    "    (\"def_eff_hometeam\", \"def_eff_awayteam\"),\n",
    "    (\"adj_off_eff_hometeam\", \"adj_off_eff_awayteam\"),\n",
    "    (\"adj_def_eff_hometeam\", \"adj_def_eff_awayteam\"),\n",
    "    (\"efg_pct_hometeam\", \"efg_pct_awayteam\"),\n",
    "    (\"ts_pct_hometeam\", \"ts_pct_awayteam\"),\n",
    "    (\"threepar_hometeam\", \"threepar_awayteam\"),\n",
    "    (\"ftr_hometeam\", \"ftr_awayteam\"),\n",
    "    (\"luck_hometeam\", \"luck_awayteam\"),\n",
    "    (\"net_rating_hometeam\",\"net_rating_awayteam\"),\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    if a in df.columns and b in df.columns:\n",
    "        df[a.replace(\"_hometeam\",\"_diff\")] = df[a] - df[b]\n",
    "\n",
    "# Select columns to keep\n",
    "keep = [\n",
    "    \"home_team\",\"away_team\",\"home_score\",\"away_score\",\"point_spread\",\n",
    "    \"is_conference\",\"is_neutral\",\"home_advantage\"  # optional if computed\n",
    "] + [c for c in df.columns if c.endswith(\"_diff\")]\n",
    "\n",
    "df = df[keep]\n",
    "df.to_csv(POST_DIFF_FILE, index=False)\n",
    "print(f\"âœ… Saved diff dataset â†’ {POST_DIFF_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85826c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added interaction terms â†’ post_info_2023_diff.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optional nonlinearity: add interaction terms to post_<YEAR>_diff.csv\n",
    "- rating_luck_interaction = net_rating_diff * luck_diff\n",
    "- shooting_strength_interaction = net_rating_diff * efg_pct_diff\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(POST_DIFF_FILE)\n",
    "for req in [\"net_rating_diff\",\"luck_diff\",\"efg_pct_diff\"]:\n",
    "    if req not in df.columns:\n",
    "        raise ValueError(f\"Missing {req} in {POST_DIFF_FILE}\")\n",
    "\n",
    "df[\"rating_luck_interaction\"] = df[\"net_rating_diff\"] * df[\"luck_diff\"]\n",
    "df[\"shooting_strength_interaction\"] = df[\"net_rating_diff\"] * df[\"efg_pct_diff\"]\n",
    "df.to_csv(POST_DIFF_FILE, index=False)\n",
    "print(f\"âœ… Added interaction terms â†’ {POST_DIFF_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1b54005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 9 features: ['net_rating_diff', 'efg_pct_diff', 'luck_diff', 'adj_off_eff_diff', 'home_advantage', 'rating_luck_interaction', 'shooting_strength_interaction', 'is_conference', 'is_neutral']\n",
      "âœ… Train size: 851 | Test size: 365\n"
     ]
    }
   ],
   "source": [
    "# === 1) SETUP & SPLIT ================================================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load diff dataset from your global POST_DIFF_FILE\n",
    "df = pd.read_csv(POST_DIFF_FILE)\n",
    "\n",
    "# Choose features & target\n",
    "features = [\n",
    "    \"net_rating_diff\",\n",
    "    \"efg_pct_diff\",\n",
    "    \"luck_diff\",\n",
    "    \"adj_off_eff_diff\",\n",
    "    \"home_advantage\",               \n",
    "    \"rating_luck_interaction\",      \n",
    "    \"shooting_strength_interaction\",\n",
    "    \"is_conference\",\n",
    "    \"is_neutral\",\n",
    "]\n",
    "# Keep only columns that actually exist\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "TARGET = \"point_spread\"\n",
    "df = df.dropna(subset=features + [TARGET])\n",
    "\n",
    "# Split features/target\n",
    "X = df[features]\n",
    "y = df[TARGET]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Scale (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ… Using {len(features)} features: {features}\")\n",
    "print(f\"âœ… Train size: {X_train.shape[0]} | Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d504e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ€ Model Performance Comparison:\n",
      "           Model      MAE       RÂ²\n",
      "           Ridge 7.595268 0.590218\n",
      "      ElasticNet 7.596620 0.590198\n",
      "           Lasso 7.597571 0.590186\n",
      "LinearRegression 7.598031 0.590158\n",
      "  HuberRegressor 7.620362 0.586386\n",
      "GradientBoosting 7.902957 0.568679\n",
      "    RandomForest 8.032374 0.550151\n",
      "\n",
      "ðŸŒŸ Best by MAE: Ridge\n"
     ]
    }
   ],
   "source": [
    "# === 2) MODEL TRAINING ===============================================\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.001, max_iter=10000),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000),\n",
    "    \"HuberRegressor\": HuberRegressor(epsilon=1.35, max_iter=1000),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=42),\n",
    "}\n",
    "\n",
    "results, trained = [], {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained[name] = model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"RÂ²\": r2_score(y_test, y_pred),\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(results).sort_values(\"MAE\").reset_index(drop=True)\n",
    "print(\"\\nðŸ€ Model Performance Comparison:\")\n",
    "print(res.to_string(index=False))\n",
    "\n",
    "# Keep the best model name for analysis chunk\n",
    "best_model_name = res.iloc[0][\"Model\"]\n",
    "print(f\"\\nðŸŒŸ Best by MAE: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ccfe375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Feature correlation matrix (train set):\n",
      "                               net_rating_diff  efg_pct_diff  luck_diff  \\\n",
      "net_rating_diff                          1.000         0.706     -0.539   \n",
      "efg_pct_diff                             0.706         1.000     -0.363   \n",
      "luck_diff                               -0.539        -0.363      1.000   \n",
      "adj_off_eff_diff                         0.811         0.856     -0.414   \n",
      "home_advantage                           0.838         0.619     -0.464   \n",
      "rating_luck_interaction                  0.375         0.267     -0.392   \n",
      "shooting_strength_interaction           -0.646        -0.693      0.223   \n",
      "is_conference                              NaN           NaN        NaN   \n",
      "is_neutral                               0.123         0.127     -0.113   \n",
      "\n",
      "                               adj_off_eff_diff  home_advantage  \\\n",
      "net_rating_diff                           0.811           0.838   \n",
      "efg_pct_diff                              0.856           0.619   \n",
      "luck_diff                                -0.414          -0.464   \n",
      "adj_off_eff_diff                          1.000           0.712   \n",
      "home_advantage                            0.712           1.000   \n",
      "rating_luck_interaction                   0.307           0.361   \n",
      "shooting_strength_interaction            -0.665          -0.605   \n",
      "is_conference                               NaN             NaN   \n",
      "is_neutral                                0.111           0.106   \n",
      "\n",
      "                               rating_luck_interaction  \\\n",
      "net_rating_diff                                  0.375   \n",
      "efg_pct_diff                                     0.267   \n",
      "luck_diff                                       -0.392   \n",
      "adj_off_eff_diff                                 0.307   \n",
      "home_advantage                                   0.361   \n",
      "rating_luck_interaction                          1.000   \n",
      "shooting_strength_interaction                   -0.486   \n",
      "is_conference                                      NaN   \n",
      "is_neutral                                       0.115   \n",
      "\n",
      "                               shooting_strength_interaction  is_conference  \\\n",
      "net_rating_diff                                       -0.646            NaN   \n",
      "efg_pct_diff                                          -0.693            NaN   \n",
      "luck_diff                                              0.223            NaN   \n",
      "adj_off_eff_diff                                      -0.665            NaN   \n",
      "home_advantage                                        -0.605            NaN   \n",
      "rating_luck_interaction                               -0.486            NaN   \n",
      "shooting_strength_interaction                          1.000            NaN   \n",
      "is_conference                                            NaN            NaN   \n",
      "is_neutral                                            -0.090            NaN   \n",
      "\n",
      "                               is_neutral  \n",
      "net_rating_diff                     0.123  \n",
      "efg_pct_diff                        0.127  \n",
      "luck_diff                          -0.113  \n",
      "adj_off_eff_diff                    0.111  \n",
      "home_advantage                      0.106  \n",
      "rating_luck_interaction             0.115  \n",
      "shooting_strength_interaction      -0.090  \n",
      "is_conference                         NaN  \n",
      "is_neutral                          1.000  \n",
      "\n",
      "ðŸ”Ž Top coefficients (LinearRegression) on standardized features:\n",
      "net_rating_diff                  10.855644\n",
      "adj_off_eff_diff                  1.731173\n",
      "rating_luck_interaction           0.880224\n",
      "shooting_strength_interaction     0.819342\n",
      "is_neutral                        0.722292\n",
      "home_advantage                   -0.517865\n",
      "efg_pct_diff                     -0.376699\n",
      "luck_diff                        -0.142300\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (Ridge) on standardized features:\n",
      "net_rating_diff                  10.788475\n",
      "adj_off_eff_diff                  1.744701\n",
      "rating_luck_interaction           0.876918\n",
      "shooting_strength_interaction     0.808418\n",
      "is_neutral                        0.722145\n",
      "home_advantage                   -0.484471\n",
      "efg_pct_diff                     -0.372163\n",
      "luck_diff                        -0.154420\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (Lasso) on standardized features:\n",
      "net_rating_diff                  10.850361\n",
      "adj_off_eff_diff                  1.724110\n",
      "rating_luck_interaction           0.878649\n",
      "shooting_strength_interaction     0.817293\n",
      "is_neutral                        0.721299\n",
      "home_advantage                   -0.511770\n",
      "efg_pct_diff                     -0.370597\n",
      "luck_diff                        -0.142298\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (ElasticNet) on standardized features:\n",
      "net_rating_diff                  10.824333\n",
      "adj_off_eff_diff                  1.733443\n",
      "rating_luck_interaction           0.878026\n",
      "shooting_strength_interaction     0.813672\n",
      "is_neutral                        0.721732\n",
      "home_advantage                   -0.500568\n",
      "efg_pct_diff                     -0.371709\n",
      "luck_diff                        -0.147478\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸ”Ž Top coefficients (HuberRegressor) on standardized features:\n",
      "net_rating_diff                  10.481841\n",
      "adj_off_eff_diff                  1.876560\n",
      "rating_luck_interaction           1.250701\n",
      "shooting_strength_interaction     0.832111\n",
      "is_neutral                        0.691293\n",
      "efg_pct_diff                     -0.542142\n",
      "home_advantage                   -0.501977\n",
      "luck_diff                        -0.206918\n",
      "is_conference                     0.000000\n",
      "\n",
      "ðŸŒ² Feature importances (RandomForest):\n",
      "net_rating_diff                  0.627669\n",
      "adj_off_eff_diff                 0.071036\n",
      "home_advantage                   0.065596\n",
      "luck_diff                        0.062366\n",
      "rating_luck_interaction          0.061988\n",
      "shooting_strength_interaction    0.057762\n",
      "efg_pct_diff                     0.047940\n",
      "is_neutral                       0.005643\n",
      "is_conference                    0.000000\n",
      "\n",
      "ðŸŒ² Feature importances (GradientBoosting):\n",
      "net_rating_diff                  0.701967\n",
      "adj_off_eff_diff                 0.094955\n",
      "rating_luck_interaction          0.046405\n",
      "home_advantage                   0.041009\n",
      "efg_pct_diff                     0.039174\n",
      "luck_diff                        0.037522\n",
      "shooting_strength_interaction    0.036223\n",
      "is_neutral                       0.002744\n",
      "is_conference                    0.000000\n",
      "\n",
      "ðŸŽ¯ Permutation importance (Î”MAE when shuffled) for Ridge:\n",
      "efg_pct_diff                     0.035691\n",
      "home_advantage                   0.023963\n",
      "shooting_strength_interaction    0.014035\n",
      "is_conference                   -0.000000\n",
      "is_neutral                      -0.003308\n",
      "luck_diff                       -0.015596\n",
      "rating_luck_interaction         -0.114574\n",
      "adj_off_eff_diff                -0.462406\n",
      "net_rating_diff                 -7.169661\n"
     ]
    }
   ],
   "source": [
    "# === 3) ANALYSIS ======================================================\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nðŸ“Œ Feature correlation matrix (train set):\")\n",
    "corr = pd.DataFrame(X_train, columns=features).corr()\n",
    "print(corr.round(3))\n",
    "\n",
    "# Coefficients for linear-type models (on standardized features)\n",
    "def show_linear_coefs(name):\n",
    "    if name in trained and hasattr(trained[name], \"coef_\"):\n",
    "        coefs = pd.Series(trained[name].coef_, index=features).sort_values(key=np.abs, ascending=False)\n",
    "        print(f\"\\nðŸ”Ž Top coefficients ({name}) on standardized features:\")\n",
    "        print(coefs.to_string())\n",
    "    else:\n",
    "        print(f\"\\n({name}) has no linear coefficients to display.\")\n",
    "\n",
    "for lin in [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"HuberRegressor\"]:\n",
    "    show_linear_coefs(lin)\n",
    "\n",
    "# Tree-based importance (if available)\n",
    "def show_importance(name):\n",
    "    if name in trained and hasattr(trained[name], \"feature_importances_\"):\n",
    "        imps = pd.Series(trained[name].feature_importances_, index=features).sort_values(ascending=False)\n",
    "        print(f\"\\nðŸŒ² Feature importances ({name}):\")\n",
    "        print(imps.to_string())\n",
    "    else:\n",
    "        print(f\"\\n({name}) has no tree feature_importances_ to display.\")\n",
    "\n",
    "for tree in [\"RandomForest\", \"GradientBoosting\"]:\n",
    "    show_importance(tree)\n",
    "\n",
    "# Optional: permutation importance for the best model (more robust)\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    best_model = trained[best_model_name]\n",
    "    perm = permutation_importance(best_model, X_test_scaled, y_test, scoring=\"neg_mean_absolute_error\",\n",
    "                                  n_repeats=10, random_state=42)\n",
    "    perm_imps = pd.Series(-perm.importances_mean, index=features).sort_values(ascending=False)\n",
    "    print(f\"\\nðŸŽ¯ Permutation importance (Î”MAE when shuffled) for {best_model_name}:\")\n",
    "    print(perm_imps.to_string())\n",
    "except Exception as e:\n",
    "    print(f\"\\n(perm importance skipped) {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "114fc87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Learning Curve (MAE Â± sd)\n",
      "Size   Best_MAE  Â±sd     Ridge_MAE  Â±sd\n",
      " 10%      8.275 Â±0.255        8.275 Â±0.255\n",
      " 20%      7.810 Â±0.179        7.810 Â±0.179\n",
      " 30%      7.707 Â±0.136        7.707 Â±0.136\n",
      " 40%      7.708 Â±0.095        7.708 Â±0.095\n",
      " 50%      7.676 Â±0.054        7.676 Â±0.054\n",
      " 60%      7.594 Â±0.058        7.594 Â±0.058\n",
      " 70%      7.631 Â±0.031        7.631 Â±0.031\n",
      " 80%      7.595 Â±0.023        7.595 Â±0.023\n",
      " 90%      7.606 Â±0.012        7.606 Â±0.012\n",
      "100%      7.595 Â±0.000        7.595 Â±0.000\n",
      "\n",
      "ðŸ§­ Diagnosis:\n",
      "- Best model improvement 50%â†’100%: 0.080 MAE\n",
      "- Ridge improvement 50%â†’100%     : 0.080 MAE\n",
      "- Last-step improvement (best)    : 0.011 MAE\n",
      "\n",
      "âœ… Verdict: ðŸ§ª Curve is flat: focus on richer features (tempo/recent form/rest/travel/etc.).\n"
     ]
    }
   ],
   "source": [
    "# === 2.5) LEARNING CURVE (plug-and-play) ==============================\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import clone\n",
    "\n",
    "def learning_curve_mae(model, Xtr, ytr, Xte, yte, sizes=None, repeats=8, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains `model` on increasing fractions of (Xtr, ytr) and evaluates MAE on (Xte, yte).\n",
    "    Returns sizes, mean MAE per size, and std MAE per size.\n",
    "    \"\"\"\n",
    "    if sizes is None:\n",
    "        sizes = np.linspace(0.1, 1.0, 10)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    means, stds = [], []\n",
    "    ytr_np = ytr.to_numpy() if hasattr(ytr, \"to_numpy\") else np.asarray(ytr)\n",
    "    for s in sizes:\n",
    "        n = max(10, int(len(Xtr) * s))\n",
    "        maes = []\n",
    "        for _ in range(repeats):\n",
    "            idx = rng.choice(len(Xtr), n, replace=False)\n",
    "            m = clone(model)\n",
    "            m.fit(Xtr[idx], ytr_np[idx])\n",
    "            pred = m.predict(Xte)\n",
    "            maes.append(mean_absolute_error(yte, pred))\n",
    "        means.append(float(np.mean(maes)))\n",
    "        stds.append(float(np.std(maes)))\n",
    "    return np.array(sizes), np.array(means), np.array(stds)\n",
    "\n",
    "# Pick the best model from your leaderboard (already computed above)\n",
    "best_model = trained[best_model_name]\n",
    "\n",
    "# Run learning curves for the best model and a Ridge baseline\n",
    "sizes, best_mean, best_std = learning_curve_mae(best_model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "_,     ridge_mean, ridge_std = learning_curve_mae(Ridge(alpha=1.0), X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Pretty print\n",
    "print(\"\\nðŸ“ˆ Learning Curve (MAE Â± sd)\")\n",
    "print(\"Size   Best_MAE  Â±sd     Ridge_MAE  Â±sd\")\n",
    "for s, bm, bs, rm, rs in zip(sizes, best_mean, best_std, ridge_mean, ridge_std):\n",
    "    print(f\"{int(s*100):>3d}%   {bm:8.3f} Â±{bs:4.3f}   {rm:10.3f} Â±{rs:4.3f}\")\n",
    "\n",
    "# Heuristic verdict\n",
    "def improvement(means, sizes_arr):\n",
    "    # improvement from ~50% to 100%\n",
    "    mid_idx = np.argmin(np.abs(sizes_arr - 0.5))\n",
    "    return means[mid_idx] - means[-1]\n",
    "\n",
    "best_improve = improvement(best_mean, sizes)\n",
    "ridge_improve = improvement(ridge_mean, sizes)\n",
    "tail_slope = best_mean[-2] - best_mean[-1] if len(best_mean) > 1 else 0.0\n",
    "\n",
    "print(\"\\nðŸ§­ Diagnosis:\")\n",
    "print(f\"- Best model improvement 50%â†’100%: {best_improve:.3f} MAE\")\n",
    "print(f\"- Ridge improvement 50%â†’100%     : {ridge_improve:.3f} MAE\")\n",
    "print(f\"- Last-step improvement (best)    : {tail_slope:.3f} MAE\")\n",
    "\n",
    "if best_improve >= 0.30 or tail_slope > 0.05:\n",
    "    verdict = \"ðŸ“¦ More data likely helps (curve still descending).\"\n",
    "elif best_improve < 0.10 and abs(tail_slope) < 0.02:\n",
    "    verdict = \"ðŸ§ª Curve is flat: focus on richer features (tempo/recent form/rest/travel/etc.).\"\n",
    "else:\n",
    "    verdict = \"âš–ï¸ Mixed: modest data gains possible, but new features will likely matter more.\"\n",
    "\n",
    "print(f\"\\nâœ… Verdict: {verdict}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
